{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_lstm_colab",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bognev/BSBL-LSTM-Net/blob/master/model_lstm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKtYo163OPb2",
        "colab_type": "code",
        "outputId": "9155b8a4-efde-48d1-baf4-75841502969c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/gdrive/My\\ Drive/foo.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Hello Google Drive!"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3ad4ff74315b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hello Google Drive!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat /content/gdrive/My\\\\ Drive/foo.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'google.colab.drive' has no attribute 'umount'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baABRS9GKVnX",
        "colab_type": "code",
        "outputId": "a2e24d70-1757-4c1d-abde-e0a239c08ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from mat4py import loadmat\n",
        "# #from torchsummary import summary\n",
        "# from graphviz import Digraph\n",
        "# from torchviz import make_dot\n",
        "# from graphviz import Source\n",
        "\n",
        "import time\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "# !cat /content/gdrive/My\\ Drive/foo.txt\n",
        "\n",
        "\n",
        "\n",
        "def passthrough(x, **kwargs):\n",
        "    return x\n",
        "\n",
        "\n",
        "class BuildLstmStack(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, rnn_size, num_layers):\n",
        "        super(BuildLstmStack, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.rnn_size = rnn_size\n",
        "        self.num_layers = num_layers\n",
        "        self.all_layers = []\n",
        "        l_i2h_lst = [nn.Linear(self.input_size, 4 * self.rnn_size)]\n",
        "        l_h2h_lst = [nn.Linear(self.rnn_size, 4 * self.rnn_size)]\n",
        "        l_bn_lst = [nn.BatchNorm1d(4 * self.rnn_size)]\n",
        "        for L in range(1, self.num_layers):\n",
        "            l_i2h_lst.append(nn.Linear(self.rnn_size, 4 * self.rnn_size))\n",
        "            l_h2h_lst.append(nn.Linear(self.rnn_size, 4 * self.rnn_size))\n",
        "            l_bn_lst.append(nn.BatchNorm1d(4 * self.rnn_size))\n",
        "        self.l_i2h = nn.ModuleList(l_i2h_lst)\n",
        "        self.l_h2h = nn.ModuleList(l_h2h_lst)\n",
        "        self.l_bn  = nn.ModuleList(l_bn_lst)\n",
        "\n",
        "    def forward(self, x, prev_hs, prev_cs):\n",
        "        self.x_size = []\n",
        "        self.prev_c = 0\n",
        "        self.prev_h = 0\n",
        "        self.next_hs = []\n",
        "        self.next_cs = []\n",
        "        self.i2h = []\n",
        "        self.h2h = []\n",
        "        for L in range(self.num_layers):\n",
        "            self.prev_c = prev_cs[L]\n",
        "            self.prev_h = prev_hs[L]\n",
        "            if L == 0:\n",
        "                self.x = x\n",
        "            else:\n",
        "                self.x = self.next_hs[L - 1]\n",
        "            self.i2h.append(self.l_i2h[L](self.x))\n",
        "            self.h2h.append(self.l_h2h[L](self.prev_h))\n",
        "            all_sums = self.l_bn[L](self.i2h[L] + self.h2h[L])\n",
        "            (n1, n2, n3, n4) = all_sums.chunk(4, dim=1)  # it should return 4 tensors self.rnn_size\n",
        "            in_gate = torch.sigmoid(n1)\n",
        "            forget_gate = torch.sigmoid(n2)\n",
        "            out_gate = torch.sigmoid(n3)\n",
        "            in_transform = torch.tanh(n4)\n",
        "            next_c = forget_gate*self.prev_c + in_gate*in_transform\n",
        "            next_h = out_gate * torch.tanh(next_c)\n",
        "\n",
        "            self.next_hs.append(next_h)\n",
        "            self.next_cs.append(next_c)\n",
        "        return torch.stack(self.next_hs), torch.stack(self.next_cs)#, i2h, h2h\n",
        "\n",
        "\n",
        "class BuildLstmUnrollNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_unroll, num_layers, rnn_size, input_size):\n",
        "        super(BuildLstmUnrollNet, self).__init__()\n",
        "        self.num_unroll = num_unroll\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_size = rnn_size\n",
        "        self.input_size = input_size\n",
        "        self.outputs = []\n",
        "        self.output = []\n",
        "        self.now_h, self.now_c = [], []\n",
        "        self.buildlstmstack_lst = []\n",
        "        for i in range(0, self.num_unroll):\n",
        "            self.buildlstmstack_lst.append(BuildLstmStack(self.input_size, self.rnn_size, self.num_layers))\n",
        "        self.buildlstmstack = nn.ModuleList(self.buildlstmstack_lst)\n",
        "\n",
        "    def forward(self, x, init_states_input):\n",
        "\n",
        "        self.init_hs = []\n",
        "        self.init_cs = []\n",
        "        self.now_hs = []\n",
        "        self.now_cs = []\n",
        "        self.outputs = []\n",
        "\n",
        "        init_states = init_states_input.reshape((init_states_input.size(0),self.num_layers * 2, self.rnn_size))\n",
        "        init_states_lst = list(init_states.chunk(self.num_layers * 2,1))\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            self.init_hs.append(init_states_lst[2*i].reshape(init_states_input.size(0),self.rnn_size))\n",
        "            self.init_cs.append(init_states_lst[2*i+1].reshape(init_states_input.size(0),self.rnn_size))\n",
        "\n",
        "        self.now_hs.append(torch.stack(self.init_hs))\n",
        "        self.now_cs.append(torch.stack(self.init_cs))\n",
        "\n",
        "        for i in range(self.num_unroll):\n",
        "            self.now_h, self.now_c = self.buildlstmstack[i](x, self.now_hs[i], self.now_cs[i])\n",
        "            self.now_hs.append(self.now_h)\n",
        "            self.now_cs.append(self.now_c)\n",
        "            #self.outputs.append(torch.cat(self.now_hs[-1],1))\n",
        "            self.outputs.append(self.now_hs[i+1][-1])\n",
        "            # for L in range(self.num_layers):\n",
        "            #     setattr(self, 'hid_%d_%d' %(i, L), self.now_hs[i][L])\n",
        "            #     setattr(self, 'cell_%d_%d' %(i, L), self.now_cs[i][L])\n",
        "        for i in range(1,self.num_unroll):\n",
        "            for j in range(self.num_layers):\n",
        "                self.buildlstmstack[i].l_i2h[j].weight.data = self.buildlstmstack[0].l_i2h[j].weight.data\n",
        "                self.buildlstmstack[i].l_h2h[j].weight.data = self.buildlstmstack[0].l_h2h[j].weight.data\n",
        "                self.buildlstmstack[i].l_i2h[j].bias.data = self.buildlstmstack[0].l_i2h[j].bias.data\n",
        "                self.buildlstmstack[i].l_h2h[j].bias.data = self.buildlstmstack[0].l_h2h[j].bias.data\n",
        "                self.buildlstmstack[i].l_i2h[j].weight.grad = self.buildlstmstack[0].l_i2h[j].weight.grad\n",
        "                self.buildlstmstack[i].l_h2h[j].weight.grad = self.buildlstmstack[0].l_h2h[j].weight.grad\n",
        "                self.buildlstmstack[i].l_i2h[j].bias.grad = self.buildlstmstack[0].l_i2h[j].bias.grad\n",
        "                self.buildlstmstack[i].l_h2h[j].bias.grad = self.buildlstmstack[0].l_h2h[j].bias.grad\n",
        "        self.output = self.outputs[0]\n",
        "        for i in range(1, self.num_unroll):\n",
        "            self.output = torch.cat((self.output, self.outputs[i]),1)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class GetLstmNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_unroll, num_layers, rnn_size, output_size, input_size):\n",
        "        super(GetLstmNet,self).__init__()\n",
        "        self.num_unroll, self.num_layers, self.rnn_size, self.output_size, self.input_size  = num_unroll, num_layers, rnn_size, output_size, input_size\n",
        "        self.l_pred_l = nn.Linear(self.num_unroll * self.rnn_size, self.output_size)\n",
        "        self.lstmnet = BuildLstmUnrollNet(self.num_unroll, self.num_layers, self.rnn_size, self.input_size)\n",
        "        self.l_pred_bn = nn.BatchNorm1d(self.output_size)\n",
        "        # setattr(self, 'LstmNetLinear', self.l_pred_l)\n",
        "\n",
        "    def forward(self, x, init_states_input):\n",
        "        self.lstm_output = self.lstmnet(x, init_states_input)\n",
        "        self.pred = self.l_pred_bn(self.l_pred_l(self.lstm_output))\n",
        "        return self.pred\n",
        "\n",
        "###########Usage#######################################\n",
        "\n",
        "input_size = 20\n",
        "output_size = 50\n",
        "rnn_size = 10\n",
        "num_layers = 2\n",
        "num_unroll = 3\n",
        "#graph of net\n",
        "x = torch.rand(3,input_size)\n",
        "z = torch.zeros(3,rnn_size * num_layers * 2)\n",
        "# model = BuildLstmStack(input_size, rnn_size, num_layers)\n",
        "# init_hs = []\n",
        "# init_cs = []\n",
        "# init_states = z.reshape((z.size(0),num_layers * 2, rnn_size))\n",
        "# init_states_lst = list(init_states.chunk(num_layers * 2,1))\n",
        "# for i in range(num_layers):\n",
        "#     init_hs.append(init_states_lst[2*i].reshape(num_layers,rnn_size))\n",
        "#     init_cs.append(init_states_lst[2*i+1].reshape(num_layers,rnn_size))\n",
        "# now_hs, now_cs = model(x, init_hs, init_cs)\n",
        "# temp = make_dot((now_hs[2], now_cs[2]), params=dict(list(model.named_parameters())))\n",
        "# s = Source(temp, filename=\"BuildLstmStack.gv\", format=\"png\")\n",
        "# s.view()\n",
        "#\n",
        "# model = BuildLstmUnrollNet(num_unroll, num_layers, rnn_size, input_size)\n",
        "# out = model(x, z)\n",
        "# temp = make_dot(out, params=dict(list(model.named_parameters())+ [('x', x)]+ [('z', z)]))\n",
        "# s = Source(temp, filename=\"BuildLstmUnrollNet.gv\", format=\"png\")\n",
        "# s.view()\n",
        "#\n",
        "# model = GetLstmNet(num_unroll, num_layers, rnn_size, output_size, input_size)\n",
        "# output = model(x,z)\n",
        "# for i in range(1, num_unroll):\n",
        "#     for j in range(num_layers):\n",
        "#         model.lstmnet.buildlstmstack[i].l_i2h[j].weight = model.lstmnet.buildlstmstack[0].l_i2h[j].weight\n",
        "#         model.lstmnet.buildlstmstack[i].l_h2h[j].weight = model.lstmnet.buildlstmstack[0].l_h2h[j].weight\n",
        "#         model.lstmnet.buildlstmstack[i].l_i2h[j].bias = model.lstmnet.buildlstmstack[0].l_i2h[j].bias\n",
        "#         model.lstmnet.buildlstmstack[i].l_h2h[j].bias = model.lstmnet.buildlstmstack[0].l_h2h[j].bias\n",
        "# print(model)\n",
        "# temp = make_dot(output, params=dict(list(model.named_parameters())+ [('x', x)]+ [('z', z)]))\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "# modell = nn.Sequential()\n",
        "# modell.add_module('W0', nn.Linear(8, 16))\n",
        "# modell.add_module('tanh', nn.Tanh())\n",
        "# modell.add_module('W1', nn.Linear(16, 1))\n",
        "#\n",
        "# x = torch.randn(1,8)\n",
        "#\n",
        "# temp = make_dot(modell(x), params=dict(modell.named_parameters()))\n",
        "#\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "class MultiClassNLLCriterion(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MultiClassNLLCriterion, self).__init__()\n",
        "        self.lsm = nn.LogSoftmax(dim=1)\n",
        "        self.nll = nn.NLLLoss()\n",
        "        self.output = 0\n",
        "        self.outputs = 0\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        self.output = self.lsm(inputs)\n",
        "        shape = target.shape\n",
        "        self.outputs = 0\n",
        "        # print(self.output.shape)\n",
        "        # print(target.shape)\n",
        "        for i in range(0,shape[1]):\n",
        "            self.outputs = self.outputs + self.nll(self.output,target[:,i].squeeze())\n",
        "        return self.outputs#/shape[1]\n",
        "\n",
        "\n",
        "#match number\n",
        "def AccS(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(num_nonz) #?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).cuda()\n",
        "#     print(label.get_device())\n",
        "#     print(pred.get_device())\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, num_nonz):\n",
        "            t_score[:,i].add_(label[:,i].float().eq(pred[:,j]).float())\n",
        "    return t_score.mean()\n",
        "#loose match\n",
        "def AccL(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(20) #?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).cuda()\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, 20):\n",
        "            t_score[:,i].add_(label[:,i].float().eq(pred[:,j]).float())#t_score[:,i].add(label[:,i].eq(pred[:,j])).float()\n",
        "    return t_score.mean()\n",
        "#sctrict match\n",
        "def AccM(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(num_nonz) #?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).cuda()\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, num_nonz):\n",
        "            t_score[:,i].add_(label[:,i].float().eq(pred[:,j]).float())#t_score[:,i].add(label[:,i].eq(pred[:,j])).float()\n",
        "    return t_score.sum(1).eq(num_nonz).sum().item() * 1./ pred.shape[0]\n",
        "\n",
        "gpu = 1 # gpu id\n",
        "batch_size = 250 #10# training batch size\n",
        "lr = 0.002 # basic learning rate\n",
        "lr_decay_startpoint = 250 #learning rate from which epoch\n",
        "num_epochs = 400 # total training epochs\n",
        "max_grad_norm = 5.0\n",
        "clip_gradient = 4.0\n",
        "\n",
        "# task related parameters\n",
        "# task: y = Ax, given A recovery sparse x from y\n",
        "dataset = 'uniform' # type of non-zero elements: uniform ([-1,-0.1]U[0.1,1]), unit (+-1)\n",
        "num_nonz = 3 # number of non-zero elemetns to recovery: 3,4,5,6,7,8,9,10\n",
        "input_size = 20 # dimension of observation vector y\n",
        "output_size = 100 # dimension of sparse vector x\n",
        "\n",
        "# model hyper parameters\n",
        "rnn_size = 800 # number of units in RNN cell\n",
        "num_layers = 2 # number of stacked RNN layers\n",
        "num_unroll = 17 # number of RNN unrolled time steps\n",
        "\n",
        "# torch.set_num_threads(16)\n",
        "# manualSeed = torch.randint(1,10000,(1,))\n",
        "# print(\"Random seed \" + str(manualSeed.item()))\n",
        "torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "train_size = 600000#100\n",
        "valid_size = 100000#10#\n",
        "valid_data = torch.zeros(valid_size, input_size).cuda()\n",
        "valid_label = torch.zeros(valid_size, num_nonz).type(torch.LongTensor).cuda()\n",
        "batch_data = torch.zeros(batch_size, input_size).cuda()\n",
        "batch_label = torch.zeros(batch_size, num_nonz).cuda() # for MultiClassNLLCriterion LOSS\n",
        "batch_zero_states = torch.zeros(batch_size, num_layers * rnn_size * 2).cuda() #init_states for lstm\n",
        "\n",
        "#AccM, AccL, Accs = 0, 0, 0\n",
        "\n",
        "\n",
        "err = 0\n",
        "\n",
        "\n",
        "\n",
        "model_all = \"model_l_\"+str(num_layers)+\"t_\"+str(num_unroll)+'_rnn_'+ str(rnn_size)\n",
        "logger_file = model_all+str(dataset)+\"_\"+str(num_nonz)+'.log'\n",
        "logger = open(logger_file, 'w')\n",
        "#for k,v in pairs(opt) do logger:write(k .. ' ' .. v ..'\\n') end\n",
        "#logger:write('network have ' .. paras:size(1) .. ' parameters' .. '\\n')\n",
        "#logger:close()\n",
        "\n",
        "# torch.manual_seed(10)\n",
        "# mat_A = torch.rand(output_size,input_size)\n",
        "mat_A = torch.load(\"/content/gdrive/My Drive/mat_A.pt\").cuda()\n",
        "\n",
        "def gen_batch(batch_size, num_nonz, mat_A):\n",
        "    # mat_A = loadmat('matrix_corr_unit_20_100.mat')\n",
        "    # mat_A = torch.FloatTensor(mat_A['A']).t()\n",
        "    #print(mat_A.shape)\n",
        "    # mat_A = torch.rand(output_size, input_size)\n",
        "    batch_X = torch.Tensor(batch_size, 100).cuda()\n",
        "    batch_n = torch.Tensor(batch_size, num_nonz).cuda()\n",
        "    bs = batch_size\n",
        "    len = int(100 / num_nonz*num_nonz)\n",
        "    perm = torch.randperm(100)[range(len)].cuda()\n",
        "#     batch_label = torch.zeros(batch_size, num_nonz).type(torch.LongTensor).cuda()  # for MultiClassNLLCriterion LOSS\n",
        "    for i in range(int(bs*num_nonz/len)):\n",
        "        perm = torch.cat((perm, torch.randperm(100)[range(len)].cuda()))\n",
        "    batch_label = perm[range(bs*num_nonz)].reshape([bs, num_nonz]).type(torch.LongTensor).cuda()\n",
        "    batch_X.zero_()\n",
        "    if dataset == 'uniform':\n",
        "        batch_n.uniform_(-0.5,0.5)\n",
        "        batch_n[batch_n.gt(0)] = batch_n[batch_n.gt(0)] + 0.1\n",
        "        batch_n[batch_n.le(0)] = batch_n[batch_n.le(0)] - 0.1\n",
        "    #\n",
        "    #print(batch_X.shape)\n",
        "#     print(batch_X.get_device())\n",
        "#     print(mat_A.get_device())\n",
        "#     print(batch_n.get_device())\n",
        "    for i in range(bs):\n",
        "        for j in range(num_nonz):\n",
        "            batch_X[i][batch_label[i][j]] = batch_n[i][j]\n",
        "    batch_data = torch.mm(batch_X, mat_A)+0.001*torch.randn(batch_size,input_size).cuda()\n",
        "    # print(batch_label.shape)\n",
        "    # print(batch_data.shape)\n",
        "    return batch_label, batch_data\n",
        "\n",
        "print(\"building validation set\")\n",
        "for i in range(0, valid_size, batch_size):\n",
        "#     mat_A = torch.rand(output_size, input_size).cuda()\n",
        "    batch_label, batch_data = gen_batch(batch_size, num_nonz, mat_A)\n",
        "    # print(batch_label.shape)\n",
        "    # print(\"batch_data shape = \" + str(batch_data.shape))\n",
        "    # print(\"valid_data shape = \" + str(valid_data.shape))\n",
        "    # print(range(i,i+batch_size-1))\n",
        "    valid_data[range(i, i + batch_size), :] = batch_data\n",
        "    valid_label[range(i, i + batch_size), :] = batch_label\n",
        "print('done')\n",
        "\n",
        "best_valid_accs = 0\n",
        "base_epoch = lr_decay_startpoint\n",
        "base_lr = lr\n",
        "optimState = {'learningRate' : 0.001, 'weigthDecay' : 0.0001}\n",
        "\n",
        "net = GetLstmNet(num_unroll, num_layers, rnn_size, output_size, input_size)\n",
        "# print(net)\n",
        "device = torch.device('cuda')\n",
        "net.to(device)\n",
        "#summary(net,[(num_layers,input_size),(num_layers,rnn_size * num_layers * 2)])\n",
        "# summary(net,[(batch_size, input_size),(batch_size, num_layers * rnn_size * 2)])\n",
        "\n",
        "# create a stochastic gradient descent optimizer\n",
        "# optimizer = optim.RMSprop(params=net.parameters(), lr=0.001, alpha=0.9, eps=1e-04, weight_decay=0.0001, momentum=0, centered=False)\n",
        "# create a loss function\n",
        "LOSS = MultiClassNLLCriterion()\n",
        "optimizer = optim.RMSprop(params=net.parameters(), lr=optimState['learningRate'],\\\n",
        "                          alpha=0.9, eps=1e-05, weight_decay=optimState['weigthDecay'], momentum=0.1, centered=False)\n",
        "\n",
        "checkpoint = torch.load( \"/content/gdrive/My Drive/model_l_2t_17_rnn_800_3.pth\")\n",
        "net.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch'] + 1\n",
        "loss = checkpoint['loss']\n",
        "# epoch=0\n",
        "\n",
        "# mat_A = torch.rand(output_size, input_size).cuda()\n",
        "for epoch in range(epoch,num_epochs):\n",
        "    \n",
        "    #learing rate self - adjustment\n",
        "    # if(epoch > 250):\n",
        "    #     optimState['learningRate'] = base_lr / (1 + 0.06 * (epoch - base_epoch))\n",
        "    #     if(epoch % 50 == 0): base_epoch = epoch; base_lr= base_lr * 0.25\n",
        "\n",
        "\n",
        "    logger = open(logger_file, 'a')\n",
        "    #train\n",
        "    train_accs = 0\n",
        "    train_accl = 0\n",
        "    train_accm = 0\n",
        "    train_err = 0\n",
        "    nbatch = 0\n",
        "    \n",
        "    net.train()\n",
        "    start = time.time()\n",
        "    for i in range(0,train_size,batch_size):        \n",
        "        batch_label, batch_data = gen_batch(batch_size, num_nonz, mat_A)\n",
        "        batch_label.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        pred_prob = net(batch_data, batch_zero_states).cuda() #0 or 1?!\n",
        "        err = LOSS(pred_prob, batch_label.cuda())\n",
        "        err.backward()\n",
        "        with torch.no_grad():\n",
        "            for name, param in net.named_parameters():\n",
        "                # print(name)\n",
        "                # print(param.grad.data)\n",
        "                param.grad.clamp_(-4.0,4.0)\n",
        "                gnorm = param.grad.norm()\n",
        "                if(gnorm > max_grad_norm):\n",
        "                    param.grad.mul_(max_grad_norm/gnorm)\n",
        "        optimizer.step()\n",
        "#         print(pred_prob.get_device())\n",
        "#         print(batch_label.get_device())\n",
        "        batch_accs = AccS(batch_label[:, range(0, num_nonz)], pred_prob.cuda().float())\n",
        "        batch_accl = AccL(batch_label[:, range(0, num_nonz)], pred_prob.cuda().float())\n",
        "        batch_accm = AccM(batch_label[:, range(0, num_nonz)], pred_prob.cuda().float())\n",
        "        train_accs = train_accs + batch_accs.item()\n",
        "        train_accl = train_accl + batch_accl.item()\n",
        "        train_accm = train_accm + batch_accm\n",
        "        train_err = train_err + err.item()\n",
        "        nbatch = nbatch + 1\n",
        "        if (nbatch) % 512 == 1:\n",
        "            print(\"Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" {:.4} {:.4} {:.4} loss = {:.4}\".format(batch_accs, batch_accl,\n",
        "                                                                                            batch_accm, err.item()))        \n",
        "    end = time.time()\n",
        "    print(\"Train [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\".format(epoch, end - start, \\\n",
        "                                                                        train_accs / nbatch, train_accl / nbatch,\\\n",
        "                                                                        train_accm / nbatch, train_err / nbatch))\n",
        "    logger.write(\"Train [{}] Time {:.4} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\\n\".format(epoch, end - start, \\\n",
        "                                                                        train_accs / nbatch, train_accl / nbatch,\\\n",
        "                                                                        train_accm / nbatch, train_err / nbatch))\n",
        "\n",
        "    #eval\n",
        "    nbatch = 0\n",
        "    valid_accs = 0\n",
        "    valid_accl = 0\n",
        "    valid_accm = 0\n",
        "    valid_err = 0\n",
        "    start = time.time()\n",
        "    net.eval()\n",
        "    for i in range(0,valid_size,batch_size):\n",
        "        batch_data = valid_data[range(i, i + batch_size),:]\n",
        "        batch_label[:,range(0, num_nonz)] = valid_label[range(i, i + batch_size), :]\n",
        "        pred_prob = net(batch_data,batch_zero_states)\n",
        "        err = LOSS(pred_prob, batch_label)\n",
        "        batch_accs = AccS(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        batch_accl = AccL(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        batch_accm = AccM(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        valid_accs = valid_accs + batch_accs.item()\n",
        "        valid_accl = valid_accl + batch_accl.item()\n",
        "        valid_accm = valid_accm + batch_accm\n",
        "        valid_err = valid_err + err.item()\n",
        "        nbatch = nbatch + 1\n",
        "#         if (nbatch+99) % 100 == 0:\n",
        "#             print(\"Eval Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" {:.4} {:.4} {:.4} loss = {:.4}\".format(batch_accs, batch_accl,\n",
        "#                                                                                             batch_accm, err.item()))\n",
        "    end = time.time()\n",
        "    print(\"Valid [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\".format(epoch, end - start, \\\n",
        "                                                                        valid_accs / nbatch, valid_accl / nbatch,\\\n",
        "                                                                        valid_accm / nbatch, valid_err / nbatch))\n",
        "    logger.write(\"Valid [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\\n\".format(epoch, end - start, \\\n",
        "                                                                        valid_accs / nbatch, valid_accl / nbatch,\\\n",
        "                                                                        train_accm / nbatch, valid_err / nbatch))\n",
        "    # if(valid_accs > best_valid_accs):\n",
        "    #     best_valid_accs = valid_accs\n",
        "    #     print(\"saving model\")\n",
        "    #     logger.write('saving model\\n')\n",
        "    #     checkpoint = {'epoch': epoch,\n",
        "    #                   'model_state_dict': net.state_dict(),\n",
        "    #                   'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #                   'loss': err.item()}\n",
        "    #     # torch.save(checkpoint, 'checkpoint.pth')\n",
        "    #     torch.save(checkpoint, \"./checkpoints/\"+model_all+\"_\"+str(num_nonz)+\".pth\") #or torch.save(net, PATH)\n",
        "    #     #net.load_state_dict(torch.load(PATH)) # or the_model = torch.load(PATH)\n",
        "\n",
        "    # if(epoch % 2 == 0):\n",
        "    print(\"saving model\")\n",
        "    logger.write('saving model\\n')\n",
        "    checkpoint = {'epoch': epoch, \\\n",
        "                  'model_state_dict': net.state_dict(), \\\n",
        "                  'optimizer_state_dict': optimizer.state_dict(), \\\n",
        "                  'loss': err.item()}\n",
        "    torch.save(checkpoint, \"/content/gdrive/My Drive/\" + model_all + \"_\" + str(num_nonz) + \".pth\")  # or torch.save(net, PATH)\n",
        "    logger.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "building validation set\n",
            "done\n",
            "Epoch 32 Batch 1 0.6867 0.9147 0.224 loss = 7.067\n",
            "Epoch 32 Batch 513 0.6933 0.9227 0.236 loss = 7.018\n",
            "Epoch 32 Batch 1025 0.7147 0.9427 0.3 loss = 6.716\n",
            "Epoch 32 Batch 1537 0.7133 0.924 0.28 loss = 6.869\n",
            "Epoch 32 Batch 2049 0.7013 0.924 0.264 loss = 6.89\n",
            "Train [32] Time 430.10230112075806 s-acc 0.7117 l-acc 0.9251 m-acc 0.2827 err 6.881\n",
            "Valid [32] Time 13.756523847579956 s-acc 0.6461 l-acc 0.8903 m-acc 0.2025 err 7.607\n",
            "saving model\n",
            "Epoch 33 Batch 1 0.6853 0.9173 0.22 loss = 7.059\n",
            "Epoch 33 Batch 513 0.7 0.9093 0.272 loss = 7.082\n",
            "Epoch 33 Batch 1025 0.7253 0.9293 0.288 loss = 6.872\n",
            "Epoch 33 Batch 1537 0.7333 0.9293 0.328 loss = 6.642\n",
            "Epoch 33 Batch 2049 0.7213 0.9267 0.324 loss = 6.792\n",
            "Train [33] Time 430.33854484558105 s-acc 0.7134 l-acc 0.9258 m-acc 0.2853 err 6.863\n",
            "Valid [33] Time 13.552678346633911 s-acc 0.6748 l-acc 0.9052 m-acc 0.2329 err 7.27\n",
            "saving model\n",
            "Epoch 34 Batch 1 0.7227 0.932 0.308 loss = 6.705\n",
            "Epoch 34 Batch 513 0.7133 0.9307 0.304 loss = 6.896\n",
            "Epoch 34 Batch 1025 0.692 0.9213 0.28 loss = 7.051\n",
            "Epoch 34 Batch 1537 0.68 0.924 0.236 loss = 7.04\n",
            "Epoch 34 Batch 2049 0.7413 0.9267 0.332 loss = 6.707\n",
            "Train [34] Time 431.3234450817108 s-acc 0.7141 l-acc 0.9263 m-acc 0.287 err 6.851\n",
            "Valid [34] Time 13.546403408050537 s-acc 0.6653 l-acc 0.9018 m-acc 0.2201 err 7.364\n",
            "saving model\n",
            "Epoch 35 Batch 1 0.7373 0.9187 0.312 loss = 6.779\n",
            "Epoch 35 Batch 513 0.7013 0.9253 0.276 loss = 6.882\n",
            "Epoch 35 Batch 1025 0.7253 0.9227 0.312 loss = 6.899\n",
            "Epoch 35 Batch 1537 0.688 0.9093 0.24 loss = 7.138\n",
            "Epoch 35 Batch 2049 0.728 0.936 0.308 loss = 6.682\n",
            "Train [35] Time 430.16725063323975 s-acc 0.7153 l-acc 0.9266 m-acc 0.2894 err 6.841\n",
            "Valid [35] Time 13.559233665466309 s-acc 0.6587 l-acc 0.8928 m-acc 0.2117 err 7.482\n",
            "saving model\n",
            "Epoch 36 Batch 1 0.712 0.9347 0.284 loss = 6.837\n",
            "Epoch 36 Batch 513 0.7267 0.9187 0.272 loss = 6.904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA283b1AsmC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgyfzFfNPvH6",
        "colab_type": "code",
        "outputId": "08672896-f1e9-47ec-9730-c36ec8e8752d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# import torch\n",
        "!~\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "import torch\n",
        "# mat_A = torch.load('mat_A.pt')\n",
        "# print(mat_A.shape)\n",
        "# print(mat_A[1,:])\n",
        "# torch.save(mat_A,\"/content/gdrive/My Drive/mat_A_test.pth\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /root: Is a directory\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}