{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bognev/BSBL-LSTM-Net/blob/master/model_lstm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baABRS9GKVnX",
        "colab_type": "code",
        "outputId": "e8ade345-c7d9-4481-ace2-d6969048ecb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1684
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from mat4py import loadmat\n",
        "# #from torchsummary import summary\n",
        "# from graphviz import Digraph\n",
        "# from torchviz import make_dot\n",
        "# from graphviz import Source\n",
        "\n",
        "import time\n",
        "\n",
        "def passthrough(x, **kwargs):\n",
        "    return x\n",
        "\n",
        "\n",
        "class BuildLstmStack(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, rnn_size, num_layers):\n",
        "        super(BuildLstmStack, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.rnn_size = rnn_size\n",
        "        self.num_layers = num_layers\n",
        "        self.all_layers = []\n",
        "        l_i2h_lst = [nn.Linear(self.input_size, 4 * self.rnn_size)]\n",
        "        l_h2h_lst = [nn.Linear(self.rnn_size, 4 * self.rnn_size)]\n",
        "        l_bn_lst = [nn.BatchNorm1d(4 * self.rnn_size)]\n",
        "        for L in range(1, self.num_layers):\n",
        "            l_i2h_lst.append(nn.Linear(self.rnn_size, 4 * self.rnn_size))\n",
        "            l_h2h_lst.append(nn.Linear(self.rnn_size, 4 * self.rnn_size))\n",
        "            l_bn_lst.append(nn.BatchNorm1d(4 * self.rnn_size))\n",
        "        self.l_i2h = nn.ModuleList(l_i2h_lst)\n",
        "        self.l_h2h = nn.ModuleList(l_h2h_lst)\n",
        "        self.l_bn  = nn.ModuleList(l_bn_lst)\n",
        "\n",
        "    def forward(self, x, prev_hs, prev_cs):\n",
        "        self.x_size = []\n",
        "        self.prev_c = 0\n",
        "        self.prev_h = 0\n",
        "        self.next_hs = []\n",
        "        self.next_cs = []\n",
        "        self.i2h = []\n",
        "        self.h2h = []\n",
        "        for L in range(self.num_layers):\n",
        "            self.prev_c = prev_cs[L]\n",
        "            self.prev_h = prev_hs[L]\n",
        "            if L == 0:\n",
        "                self.x = x\n",
        "            else:\n",
        "                self.x = self.next_hs[L - 1]\n",
        "            self.i2h.append(self.l_i2h[L](self.x))\n",
        "            self.h2h.append(self.l_h2h[L](self.prev_h))\n",
        "            all_sums = self.l_bn[L](self.i2h[L] + self.h2h[L])\n",
        "            (n1, n2, n3, n4) = all_sums.chunk(4, dim=1)  # it should return 4 tensors self.rnn_size\n",
        "            in_gate = torch.sigmoid(n1)\n",
        "            forget_gate = torch.sigmoid(n2)\n",
        "            out_gate = torch.sigmoid(n3)\n",
        "            in_transform = torch.tanh(n4)\n",
        "            next_c = forget_gate*self.prev_c + in_gate*in_transform\n",
        "            next_h = out_gate * torch.tanh(next_c)\n",
        "\n",
        "            self.next_hs.append(next_h)\n",
        "            self.next_cs.append(next_c)\n",
        "        return torch.stack(self.next_hs), torch.stack(self.next_cs)#, i2h, h2h\n",
        "\n",
        "\n",
        "class BuildLstmUnrollNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_unroll, num_layers, rnn_size, input_size):\n",
        "        super(BuildLstmUnrollNet, self).__init__()\n",
        "        self.num_unroll = num_unroll\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_size = rnn_size\n",
        "        self.input_size = input_size\n",
        "        self.outputs = []\n",
        "        self.output = []\n",
        "        self.now_h, self.now_c = [], []\n",
        "        self.buildlstmstack_lst = []\n",
        "        for i in range(0, self.num_unroll):\n",
        "            self.buildlstmstack_lst.append(BuildLstmStack(self.input_size, self.rnn_size, self.num_layers))\n",
        "        self.buildlstmstack = nn.ModuleList(self.buildlstmstack_lst)\n",
        "\n",
        "    def forward(self, x, init_states_input):\n",
        "\n",
        "        self.init_hs = []\n",
        "        self.init_cs = []\n",
        "        self.now_hs = []\n",
        "        self.now_cs = []\n",
        "        self.outputs = []\n",
        "\n",
        "        init_states = init_states_input.reshape((init_states_input.size(0),self.num_layers * 2, self.rnn_size))\n",
        "        init_states_lst = list(init_states.chunk(self.num_layers * 2,1))\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            self.init_hs.append(init_states_lst[2*i].reshape(init_states_input.size(0),self.rnn_size))\n",
        "            self.init_cs.append(init_states_lst[2*i+1].reshape(init_states_input.size(0),self.rnn_size))\n",
        "\n",
        "        self.now_hs.append(torch.stack(self.init_hs))\n",
        "        self.now_cs.append(torch.stack(self.init_cs))\n",
        "\n",
        "        for i in range(self.num_unroll):\n",
        "            self.now_h, self.now_c = self.buildlstmstack[i](x, self.now_hs[i], self.now_cs[i])\n",
        "            self.now_hs.append(self.now_h)\n",
        "            self.now_cs.append(self.now_c)\n",
        "            #self.outputs.append(torch.cat(self.now_hs[-1],1))\n",
        "            self.outputs.append(self.now_hs[i+1][-1])\n",
        "            # for L in range(self.num_layers):\n",
        "            #     setattr(self, 'hid_%d_%d' %(i, L), self.now_hs[i][L])\n",
        "            #     setattr(self, 'cell_%d_%d' %(i, L), self.now_cs[i][L])\n",
        "        for i in range(1,self.num_unroll):\n",
        "            for j in range(self.num_layers):\n",
        "                self.buildlstmstack[i].l_i2h[j].weight.data = self.buildlstmstack[0].l_i2h[j].weight.data\n",
        "                self.buildlstmstack[i].l_h2h[j].weight.data = self.buildlstmstack[0].l_h2h[j].weight.data\n",
        "                self.buildlstmstack[i].l_i2h[j].bias.data = self.buildlstmstack[0].l_i2h[j].bias.data\n",
        "                self.buildlstmstack[i].l_h2h[j].bias.data = self.buildlstmstack[0].l_h2h[j].bias.data\n",
        "                self.buildlstmstack[i].l_i2h[j].weight.grad = self.buildlstmstack[0].l_i2h[j].weight.grad\n",
        "                self.buildlstmstack[i].l_h2h[j].weight.grad = self.buildlstmstack[0].l_h2h[j].weight.grad\n",
        "                self.buildlstmstack[i].l_i2h[j].bias.grad = self.buildlstmstack[0].l_i2h[j].bias.grad\n",
        "                self.buildlstmstack[i].l_h2h[j].bias.grad = self.buildlstmstack[0].l_h2h[j].bias.grad\n",
        "        self.output = self.outputs[0]\n",
        "        for i in range(1, self.num_unroll):\n",
        "            self.output = torch.cat((self.output, self.outputs[i]),1)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class GetLstmNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_unroll, num_layers, rnn_size, output_size, input_size):\n",
        "        super(GetLstmNet,self).__init__()\n",
        "        self.num_unroll, self.num_layers, self.rnn_size, self.output_size, self.input_size  = num_unroll, num_layers, rnn_size, output_size, input_size\n",
        "        self.l_pred_l = nn.Linear(self.num_unroll * self.rnn_size, self.output_size)\n",
        "        self.lstmnet = BuildLstmUnrollNet(self.num_unroll, self.num_layers, self.rnn_size, self.input_size)\n",
        "        self.l_pred_bn = nn.BatchNorm1d(self.output_size)\n",
        "        # setattr(self, 'LstmNetLinear', self.l_pred_l)\n",
        "\n",
        "    def forward(self, x, init_states_input):\n",
        "        self.lstm_output = self.lstmnet(x, init_states_input)\n",
        "        self.pred = self.l_pred_bn(self.l_pred_l(self.lstm_output))\n",
        "        return self.pred\n",
        "\n",
        "###########Usage#######################################\n",
        "\n",
        "input_size = 20\n",
        "output_size = 50\n",
        "rnn_size = 10\n",
        "num_layers = 2\n",
        "num_unroll = 3\n",
        "#graph of net\n",
        "x = torch.rand(3,input_size)\n",
        "z = torch.zeros(3,rnn_size * num_layers * 2)\n",
        "# model = BuildLstmStack(input_size, rnn_size, num_layers)\n",
        "# init_hs = []\n",
        "# init_cs = []\n",
        "# init_states = z.reshape((z.size(0),num_layers * 2, rnn_size))\n",
        "# init_states_lst = list(init_states.chunk(num_layers * 2,1))\n",
        "# for i in range(num_layers):\n",
        "#     init_hs.append(init_states_lst[2*i].reshape(num_layers,rnn_size))\n",
        "#     init_cs.append(init_states_lst[2*i+1].reshape(num_layers,rnn_size))\n",
        "# now_hs, now_cs = model(x, init_hs, init_cs)\n",
        "# temp = make_dot((now_hs[2], now_cs[2]), params=dict(list(model.named_parameters())))\n",
        "# s = Source(temp, filename=\"BuildLstmStack.gv\", format=\"png\")\n",
        "# s.view()\n",
        "#\n",
        "# model = BuildLstmUnrollNet(num_unroll, num_layers, rnn_size, input_size)\n",
        "# out = model(x, z)\n",
        "# temp = make_dot(out, params=dict(list(model.named_parameters())+ [('x', x)]+ [('z', z)]))\n",
        "# s = Source(temp, filename=\"BuildLstmUnrollNet.gv\", format=\"png\")\n",
        "# s.view()\n",
        "#\n",
        "# model = GetLstmNet(num_unroll, num_layers, rnn_size, output_size, input_size)\n",
        "# output = model(x,z)\n",
        "# for i in range(1, num_unroll):\n",
        "#     for j in range(num_layers):\n",
        "#         model.lstmnet.buildlstmstack[i].l_i2h[j].weight = model.lstmnet.buildlstmstack[0].l_i2h[j].weight\n",
        "#         model.lstmnet.buildlstmstack[i].l_h2h[j].weight = model.lstmnet.buildlstmstack[0].l_h2h[j].weight\n",
        "#         model.lstmnet.buildlstmstack[i].l_i2h[j].bias = model.lstmnet.buildlstmstack[0].l_i2h[j].bias\n",
        "#         model.lstmnet.buildlstmstack[i].l_h2h[j].bias = model.lstmnet.buildlstmstack[0].l_h2h[j].bias\n",
        "# print(model)\n",
        "# temp = make_dot(output, params=dict(list(model.named_parameters())+ [('x', x)]+ [('z', z)]))\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "# modell = nn.Sequential()\n",
        "# modell.add_module('W0', nn.Linear(8, 16))\n",
        "# modell.add_module('tanh', nn.Tanh())\n",
        "# modell.add_module('W1', nn.Linear(16, 1))\n",
        "#\n",
        "# x = torch.randn(1,8)\n",
        "#\n",
        "# temp = make_dot(modell(x), params=dict(modell.named_parameters()))\n",
        "#\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "class MultiClassNLLCriterion(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MultiClassNLLCriterion, self).__init__()\n",
        "        self.lsm = nn.LogSoftmax(dim=1)\n",
        "        self.nll = nn.NLLLoss()\n",
        "        self.output = 0\n",
        "        self.outputs = 0\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        self.output = self.lsm(inputs)\n",
        "        shape = target.shape\n",
        "        self.outputs = 0\n",
        "        # print(self.output.shape)\n",
        "        # print(target.shape)\n",
        "        for i in range(0,shape[1]):\n",
        "            self.outputs = self.outputs + self.nll(self.output,target[:,i].squeeze())\n",
        "        return self.outputs#/shape[1]\n",
        "\n",
        "\n",
        "#match number\n",
        "def AccS(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(num_nonz) #?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).cuda()\n",
        "#     print(label.get_device())\n",
        "#     print(pred.get_device())\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, num_nonz):\n",
        "            t_score[:,i].add_(label[:,i].float().eq(pred[:,j]).float())\n",
        "    return t_score.mean()\n",
        "#loose match\n",
        "def AccL(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(20) #?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).cuda()\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, 20):\n",
        "            t_score[:,i].add_(label[:,i].float().eq(pred[:,j]).float())#t_score[:,i].add(label[:,i].eq(pred[:,j])).float()\n",
        "    return t_score.mean()\n",
        "#sctrict match\n",
        "def AccM(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(num_nonz) #?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).cuda()\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, num_nonz):\n",
        "            t_score[:,i].add_(label[:,i].float().eq(pred[:,j]).float())#t_score[:,i].add(label[:,i].eq(pred[:,j])).float()\n",
        "    return t_score.sum(1).eq(num_nonz).sum().item() * 1./ pred.shape[0]\n",
        "\n",
        "gpu = 1 # gpu id\n",
        "batch_size = 250 #10# training batch size\n",
        "lr = 0.002 # basic learning rate\n",
        "lr_decay_startpoint = 250 #learning rate from which epoch\n",
        "num_epochs = 500 # total training epochs\n",
        "max_grad_norm = 5.0\n",
        "clip_gradient = 4.0\n",
        "\n",
        "# task related parameters\n",
        "# task: y = Ax, given A recovery sparse x from y\n",
        "dataset = 'uniform' # type of non-zero elements: uniform ([-1,-0.1]U[0.1,1]), unit (+-1)\n",
        "num_nonz = 3 # number of non-zero elemetns to recovery: 3,4,5,6,7,8,9,10\n",
        "input_size = 20 # dimension of observation vector y\n",
        "output_size = 100 # dimension of sparse vector x\n",
        "\n",
        "# model hyper parameters\n",
        "rnn_size = 425 # number of units in RNN cell\n",
        "num_layers = 2 # number of stacked RNN layers\n",
        "num_unroll = 5 # number of RNN unrolled time steps\n",
        "\n",
        "# torch.set_num_threads(16)\n",
        "# manualSeed = torch.randint(1,10000,(1,))\n",
        "# print(\"Random seed \" + str(manualSeed.item()))\n",
        "torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "train_size = 600000#100\n",
        "valid_size = 100000#10#\n",
        "valid_data = torch.zeros(valid_size, input_size).cuda()\n",
        "valid_label = torch.zeros(valid_size, num_nonz).type(torch.LongTensor).cuda()\n",
        "batch_data = torch.zeros(batch_size, input_size).cuda()\n",
        "batch_label = torch.zeros(batch_size, num_nonz).cuda() # for MultiClassNLLCriterion LOSS\n",
        "batch_zero_states = torch.zeros(batch_size, num_layers * rnn_size * 2).cuda() #init_states for lstm\n",
        "\n",
        "#AccM, AccL, Accs = 0, 0, 0\n",
        "\n",
        "\n",
        "err = 0\n",
        "\n",
        "\n",
        "\n",
        "model_all = \"model_l_\"+str(num_layers)+\"t_\"+str(num_unroll)+'_rnn_'+ str(rnn_size)\n",
        "logger_file = model_all+str(dataset)+\"_\"+str(num_nonz)+'.log'\n",
        "logger = open(logger_file, 'w')\n",
        "#for k,v in pairs(opt) do logger:write(k .. ' ' .. v ..'\\n') end\n",
        "#logger:write('network have ' .. paras:size(1) .. ' parameters' .. '\\n')\n",
        "#logger:close()\n",
        "\n",
        "# torch.manual_seed(10)\n",
        "# mat_A = torch.rand(output_size,input_size)\n",
        "\n",
        "def gen_batch(batch_size, num_nonz, mat_A):\n",
        "    # mat_A = loadmat('matrix_corr_unit_20_100.mat')\n",
        "    # mat_A = torch.FloatTensor(mat_A['A']).t()\n",
        "    #print(mat_A.shape)\n",
        "    # mat_A = torch.rand(output_size, input_size)\n",
        "    batch_X = torch.Tensor(batch_size, 100).cuda()\n",
        "    batch_n = torch.Tensor(batch_size, num_nonz).cuda()\n",
        "    bs = batch_size\n",
        "    len = int(100 / num_nonz*num_nonz)\n",
        "    perm = torch.randperm(100)[range(len)].cuda()\n",
        "#     batch_label = torch.zeros(batch_size, num_nonz).type(torch.LongTensor).cuda()  # for MultiClassNLLCriterion LOSS\n",
        "    for i in range(int(bs*num_nonz/len)):\n",
        "        perm = torch.cat((perm, torch.randperm(100)[range(len)].cuda()))\n",
        "    batch_label = perm[range(bs*num_nonz)].reshape([bs, num_nonz]).type(torch.LongTensor).cuda()\n",
        "    batch_X.zero_()\n",
        "    if dataset == 'uniform':\n",
        "        batch_n.uniform_(-0.4,0.4)\n",
        "        batch_n[batch_n.gt(0)] = batch_n[batch_n.gt(0)] + 0.1\n",
        "        batch_n[batch_n.le(0)] = batch_n[batch_n.le(0)] - 0.1\n",
        "    #\n",
        "    #print(batch_X.shape)\n",
        "#     print(batch_X.get_device())\n",
        "#     print(mat_A.get_device())\n",
        "#     print(batch_n.get_device())\n",
        "    for i in range(bs):\n",
        "        for j in range(num_nonz):\n",
        "            batch_X[i][batch_label[i][j]] = batch_n[i][j]\n",
        "    batch_data = torch.mm(batch_X, mat_A)\n",
        "    # print(batch_label.shape)\n",
        "    # print(batch_data.shape)\n",
        "    return batch_label, batch_data\n",
        "\n",
        "print(\"building validation set\")\n",
        "for i in range(0, valid_size, batch_size):\n",
        "    mat_A = torch.rand(output_size, input_size).cuda()\n",
        "    batch_label, batch_data = gen_batch(batch_size, num_nonz, mat_A)\n",
        "    # print(batch_label.shape)\n",
        "    # print(\"batch_data shape = \" + str(batch_data.shape))\n",
        "    # print(\"valid_data shape = \" + str(valid_data.shape))\n",
        "    # print(range(i,i+batch_size-1))\n",
        "    valid_data[range(i, i + batch_size), :] = batch_data\n",
        "    valid_label[range(i, i + batch_size), :] = batch_label\n",
        "print('done')\n",
        "\n",
        "best_valid_accs = 0\n",
        "base_epoch = lr_decay_startpoint\n",
        "base_lr = lr\n",
        "optimState = {'learningRate' : 0.001, 'weigthDecay' : 0.0001}\n",
        "\n",
        "net = GetLstmNet(num_unroll, num_layers, rnn_size, output_size, input_size)\n",
        "# print(net)\n",
        "device = torch.device('cuda')\n",
        "net.to(device)\n",
        "#summary(net,[(num_layers,input_size),(num_layers,rnn_size * num_layers * 2)])\n",
        "# summary(net,[(batch_size, input_size),(batch_size, num_layers * rnn_size * 2)])\n",
        "\n",
        "# create a stochastic gradient descent optimizer\n",
        "# optimizer = optim.RMSprop(params=net.parameters(), lr=0.001, alpha=0.9, eps=1e-04, weight_decay=0.0001, momentum=0, centered=False)\n",
        "# create a loss function\n",
        "LOSS = MultiClassNLLCriterion()\n",
        "optimizer = optim.RMSprop(params=net.parameters(), lr=optimState['learningRate'],\\\n",
        "                          alpha=0.99, eps=1e-05, weight_decay=optimState['weigthDecay'], momentum=0.1, centered=False)\n",
        "\n",
        "# checkpoint = torch.load( \"./model_l_2t_11_rnn_425_3.pth\")\n",
        "# net.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch'] + 1\n",
        "# loss = checkpoint['loss']\n",
        "epoch=0\n",
        "\n",
        "for epoch in range(epoch,num_epochs):\n",
        "    mat_A = torch.rand(output_size, input_size).cuda()\n",
        "    #learing rate self - adjustment\n",
        "    # if(epoch > 250):\n",
        "    #     optimState['learningRate'] = base_lr / (1 + 0.06 * (epoch - base_epoch))\n",
        "    #     if(epoch % 50 == 0): base_epoch = epoch; base_lr= base_lr * 0.25\n",
        "\n",
        "\n",
        "    logger = open(logger_file, 'a')\n",
        "    #train\n",
        "    train_accs = 0\n",
        "    train_accl = 0\n",
        "    train_accm = 0\n",
        "    train_err = 0\n",
        "    nbatch = 0\n",
        "    net.train()\n",
        "    start = time.time()\n",
        "    for i in range(0,train_size,batch_size):\n",
        "        batch_label, batch_data = gen_batch(batch_size, num_nonz, mat_A)\n",
        "        batch_label.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        pred_prob = net(batch_data, batch_zero_states).cuda() #0 or 1?!\n",
        "        err = LOSS(pred_prob, batch_label.cuda())\n",
        "        err.backward()\n",
        "        with torch.no_grad():\n",
        "            for name, param in net.named_parameters():\n",
        "                # print(name)\n",
        "                # print(param.grad.data)\n",
        "                param.grad.clamp_(-4.0,4.0)\n",
        "                gnorm = param.grad.norm()\n",
        "                if(gnorm > max_grad_norm):\n",
        "                    param.grad.mul_(max_grad_norm/gnorm)\n",
        "        optimizer.step()\n",
        "#         print(pred_prob.get_device())\n",
        "#         print(batch_label.get_device())\n",
        "        batch_accs = AccS(batch_label[:, range(0, num_nonz)], pred_prob.cuda().float())\n",
        "        batch_accl = AccL(batch_label[:, range(0, num_nonz)], pred_prob.cuda().float())\n",
        "        batch_accm = AccM(batch_label[:, range(0, num_nonz)], pred_prob.cuda().float())\n",
        "        train_accs = train_accs + batch_accs.item()\n",
        "        train_accl = train_accl + batch_accl.item()\n",
        "        train_accm = train_accm + batch_accm\n",
        "        train_err = train_err + err.item()\n",
        "        nbatch = nbatch + 1\n",
        "        if (nbatch+99) % 100 == 0:\n",
        "            print(\"Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" {:.4} {:.4} {:.4} loss = {:.4}\".format(batch_accs, batch_accl,\n",
        "                                                                                            batch_accm, err.item()))\n",
        "        # print(\"Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" loss = \" + str(err.item()))\n",
        "        # if nbatch % 512 == 1:\n",
        "        #     print(\"Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \"{} {} {} loss = {}\".format(batch_accs, batch_accl, batch_accm, err.item()))\n",
        "    end = time.time()\n",
        "    print(\"Train [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\".format(epoch, end - start, \\\n",
        "                                                                        train_accs / nbatch, train_accl / nbatch,\\\n",
        "                                                                        train_accm / nbatch, train_err / nbatch))\n",
        "    logger.write(\"Train [{}] Time {:.4} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\\n\".format(epoch, end - start, \\\n",
        "                                                                        train_accs / nbatch, train_accl / nbatch,\\\n",
        "                                                                        train_accm / nbatch, train_err / nbatch))\n",
        "\n",
        "    #eval\n",
        "    nbatch = 0\n",
        "    valid_accs = 0\n",
        "    valid_accl = 0\n",
        "    valid_accm = 0\n",
        "    valid_err = 0\n",
        "    start = time.time()\n",
        "    net.eval()\n",
        "    for i in range(0,valid_size,batch_size):\n",
        "        batch_data = valid_data[range(i, i + batch_size),:]\n",
        "        batch_label[:,range(0, num_nonz)] = valid_label[range(i, i + batch_size), :]\n",
        "        pred_prob = net(batch_data,batch_zero_states)\n",
        "        err = LOSS(pred_prob, batch_label)\n",
        "        batch_accs = AccS(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        batch_accl = AccL(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        batch_accm = AccM(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        valid_accs = valid_accs + batch_accs.item()\n",
        "        valid_accl = valid_accl + batch_accl.item()\n",
        "        valid_accm = valid_accm + batch_accm\n",
        "        valid_err = valid_err + err.item()\n",
        "        nbatch = nbatch + 1\n",
        "        if (nbatch+50) % 50 == 0:\n",
        "            print(\"Eval Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" {:.4} {:.4} {:.4} loss = {:.4}\".format(batch_accs, batch_accl,\n",
        "                                                                                            batch_accm, err.item()))\n",
        "    end = time.time()\n",
        "    print(\"Valid [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\".format(epoch, end - start, \\\n",
        "                                                                        valid_accs / nbatch, valid_accl / nbatch,\\\n",
        "                                                                        valid_accm / nbatch, valid_err / nbatch))\n",
        "    logger.write(\"Valid [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\\n\".format(epoch, end - start, \\\n",
        "                                                                        valid_accs / nbatch, valid_accl / nbatch,\\\n",
        "                                                                        train_accm / nbatch, valid_err / nbatch))\n",
        "    # if(valid_accs > best_valid_accs):\n",
        "    #     best_valid_accs = valid_accs\n",
        "    #     print(\"saving model\")\n",
        "    #     logger.write('saving model\\n')\n",
        "    #     checkpoint = {'epoch': epoch,\n",
        "    #                   'model_state_dict': net.state_dict(),\n",
        "    #                   'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #                   'loss': err.item()}\n",
        "    #     # torch.save(checkpoint, 'checkpoint.pth')\n",
        "    #     torch.save(checkpoint, \"./checkpoints/\"+model_all+\"_\"+str(num_nonz)+\".pth\") #or torch.save(net, PATH)\n",
        "    #     #net.load_state_dict(torch.load(PATH)) # or the_model = torch.load(PATH)\n",
        "\n",
        "    # if(epoch % 2 == 0):\n",
        "    print(\"saving model\")\n",
        "    logger.write('saving model\\n')\n",
        "    checkpoint = {'epoch': epoch, \\\n",
        "                  'model_state_dict': net.state_dict(), \\\n",
        "                  'optimizer_state_dict': optimizer.state_dict(), \\\n",
        "                  'loss': err.item()}\n",
        "    torch.save(checkpoint, \"./\" + model_all + \"_\" + str(num_nonz) + \".pth\")  # or torch.save(net, PATH)\n",
        "    logger.close()\n",
        "    # if epoch == lr_decay_startpoint:\n",
        "    #     optimState[\"learningRate\"] = 0.001\n",
        "    #     optimState[\"weightDecay\"] = 0.001\n",
        "\n",
        "\n",
        "#print(end - start)\n",
        "\n",
        "# temp = make_dot(pred_prob, params=dict(list(net.named_parameters())+ [('batch_data', batch_data)]+ [('batch_zero_states', batch_zero_states)]))\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # run the main training loop\n",
        "# for epoch in range(epochs):\n",
        "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
        "#     data, target = data, target\n",
        "#     # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
        "#     data = data.view(-1, 28*28)\n",
        "#     optimizer.zero_grad()\n",
        "#     net_out = model(data)\n",
        "#     loss = criterion(net_out, target)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if batch_idx % log_interval == 0:\n",
        "#         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "#         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "#         100. * batch_idx / len(train_loader), loss.data[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building validation set\n",
            "done\n",
            "Epoch 0 Batch 1 0.03733 0.2093 0.0 loss = 14.32\n",
            "Epoch 0 Batch 101 0.5253 0.8387 0.064 loss = 10.06\n",
            "Epoch 0 Batch 201 0.632 0.8933 0.148 loss = 9.009\n",
            "Epoch 0 Batch 301 0.668 0.9067 0.172 loss = 8.469\n",
            "Epoch 0 Batch 401 0.7027 0.9133 0.268 loss = 7.973\n",
            "Epoch 0 Batch 501 0.7453 0.9307 0.332 loss = 7.403\n",
            "Epoch 0 Batch 601 0.728 0.9387 0.304 loss = 7.464\n",
            "Epoch 0 Batch 701 0.7653 0.944 0.364 loss = 7.095\n",
            "Epoch 0 Batch 801 0.792 0.964 0.448 loss = 6.735\n",
            "Epoch 0 Batch 901 0.8307 0.9693 0.516 loss = 6.247\n",
            "Epoch 0 Batch 1001 0.8267 0.9733 0.504 loss = 6.259\n",
            "Epoch 0 Batch 1101 0.776 0.96 0.412 loss = 6.487\n",
            "Epoch 0 Batch 1201 0.8293 0.9813 0.536 loss = 6.025\n",
            "Epoch 0 Batch 1301 0.8547 0.9827 0.592 loss = 5.805\n",
            "Epoch 0 Batch 1401 0.852 0.9707 0.596 loss = 5.857\n",
            "Epoch 0 Batch 1501 0.8587 0.9853 0.608 loss = 5.71\n",
            "Epoch 0 Batch 1601 0.8707 0.9893 0.624 loss = 5.591\n",
            "Epoch 0 Batch 1701 0.8613 0.9787 0.612 loss = 5.599\n",
            "Epoch 0 Batch 1801 0.8787 0.9907 0.664 loss = 5.491\n",
            "Epoch 0 Batch 1901 0.8653 0.992 0.616 loss = 5.492\n",
            "Epoch 0 Batch 2001 0.8827 0.984 0.68 loss = 5.454\n",
            "Epoch 0 Batch 2101 0.9027 0.988 0.712 loss = 5.237\n",
            "Epoch 0 Batch 2201 0.8773 0.9853 0.656 loss = 5.403\n",
            "Epoch 0 Batch 2301 0.8627 0.988 0.624 loss = 5.488\n",
            "Train [0] Time 136.62520909309387 s-acc 0.7886 l-acc 0.9471 m-acc 0.4749 err 6.632\n",
            "Eval Epoch 0 Batch 50 0.02133 0.1853 0.0 loss = 19.02\n",
            "Eval Epoch 0 Batch 100 0.03067 0.196 0.0 loss = 18.76\n",
            "Eval Epoch 0 Batch 150 0.03467 0.1987 0.0 loss = 18.63\n",
            "Eval Epoch 0 Batch 200 0.04667 0.2133 0.0 loss = 18.59\n",
            "Eval Epoch 0 Batch 250 0.028 0.2133 0.0 loss = 18.6\n",
            "Eval Epoch 0 Batch 300 0.03467 0.192 0.0 loss = 19.01\n",
            "Eval Epoch 0 Batch 350 0.03733 0.228 0.0 loss = 18.32\n",
            "Eval Epoch 0 Batch 400 0.032 0.204 0.0 loss = 18.56\n",
            "Valid [0] Time 4.245533466339111 s-acc 0.02998 l-acc 0.2002 m-acc 2e-05 err 18.75\n",
            "saving model\n",
            "Epoch 1 Batch 1 0.036 0.2053 0.0 loss = 18.25\n",
            "Epoch 1 Batch 101 0.5893 0.8307 0.12 loss = 8.833\n",
            "Epoch 1 Batch 201 0.6853 0.908 0.24 loss = 7.536\n",
            "Epoch 1 Batch 301 0.7227 0.9387 0.272 loss = 7.11\n",
            "Epoch 1 Batch 401 0.7747 0.9427 0.388 loss = 6.504\n",
            "Epoch 1 Batch 501 0.8 0.968 0.444 loss = 6.126\n",
            "Epoch 1 Batch 601 0.8333 0.964 0.528 loss = 6.048\n",
            "Epoch 1 Batch 701 0.812 0.9667 0.508 loss = 6.079\n",
            "Epoch 1 Batch 801 0.8307 0.9827 0.532 loss = 5.717\n",
            "Epoch 1 Batch 901 0.8373 0.976 0.552 loss = 5.719\n",
            "Epoch 1 Batch 1001 0.84 0.972 0.584 loss = 5.832\n",
            "Epoch 1 Batch 1101 0.8573 0.9773 0.604 loss = 5.592\n",
            "Epoch 1 Batch 1201 0.8613 0.988 0.608 loss = 5.44\n",
            "Epoch 1 Batch 1301 0.864 0.984 0.62 loss = 5.422\n",
            "Epoch 1 Batch 1401 0.8707 0.9893 0.628 loss = 5.309\n",
            "Epoch 1 Batch 1501 0.8587 0.9813 0.604 loss = 5.406\n",
            "Epoch 1 Batch 1601 0.8653 0.9867 0.608 loss = 5.324\n",
            "Epoch 1 Batch 1701 0.876 0.988 0.644 loss = 5.216\n",
            "Epoch 1 Batch 1801 0.8787 0.9813 0.668 loss = 5.33\n",
            "Epoch 1 Batch 1901 0.8747 0.9893 0.652 loss = 5.228\n",
            "Epoch 1 Batch 2001 0.896 0.988 0.708 loss = 5.098\n",
            "Epoch 1 Batch 2101 0.8773 0.9813 0.648 loss = 5.173\n",
            "Epoch 1 Batch 2201 0.8827 0.9907 0.672 loss = 5.087\n",
            "Epoch 1 Batch 2301 0.8973 0.9893 0.712 loss = 5.041\n",
            "Train [1] Time 139.10330939292908 s-acc 0.822 l-acc 0.9629 m-acc 0.5354 err 5.937\n",
            "Eval Epoch 1 Batch 50 0.032 0.204 0.0 loss = 19.68\n",
            "Eval Epoch 1 Batch 100 0.024 0.1987 0.0 loss = 19.9\n",
            "Eval Epoch 1 Batch 150 0.03733 0.1867 0.0 loss = 19.97\n",
            "Eval Epoch 1 Batch 200 0.028 0.1747 0.0 loss = 20.28\n",
            "Eval Epoch 1 Batch 250 0.02533 0.1947 0.0 loss = 19.99\n",
            "Eval Epoch 1 Batch 300 0.028 0.2187 0.0 loss = 19.65\n",
            "Eval Epoch 1 Batch 350 0.02 0.212 0.0 loss = 19.79\n",
            "Eval Epoch 1 Batch 400 0.024 0.1987 0.0 loss = 19.9\n",
            "Valid [1] Time 4.236928939819336 s-acc 0.02978 l-acc 0.2007 m-acc 1e-05 err 19.89\n",
            "saving model\n",
            "Epoch 2 Batch 1 0.03333 0.192 0.0 loss = 19.43\n",
            "Epoch 2 Batch 101 0.6213 0.852 0.116 loss = 8.335\n",
            "Epoch 2 Batch 201 0.6867 0.9213 0.212 loss = 7.237\n",
            "Epoch 2 Batch 301 0.728 0.944 0.284 loss = 6.785\n",
            "Epoch 2 Batch 401 0.7853 0.9533 0.416 loss = 6.361\n",
            "Epoch 2 Batch 501 0.796 0.968 0.444 loss = 6.056\n",
            "Epoch 2 Batch 601 0.8227 0.9747 0.492 loss = 5.849\n",
            "Epoch 2 Batch 701 0.8067 0.9613 0.464 loss = 6.065\n",
            "Epoch 2 Batch 801 0.8373 0.964 0.544 loss = 5.796\n",
            "Epoch 2 Batch 901 0.8587 0.9813 0.6 loss = 5.41\n",
            "Epoch 2 Batch 1001 0.8213 0.9773 0.516 loss = 5.736\n",
            "Epoch 2 Batch 1101 0.8267 0.972 0.5 loss = 5.703\n",
            "Epoch 2 Batch 1201 0.8627 0.98 0.612 loss = 5.442\n",
            "Epoch 2 Batch 1301 0.8507 0.98 0.576 loss = 5.397\n",
            "Epoch 2 Batch 1401 0.8827 0.9907 0.664 loss = 5.092\n",
            "Epoch 2 Batch 1501 0.8627 0.988 0.608 loss = 5.247\n",
            "Epoch 2 Batch 1601 0.888 0.984 0.684 loss = 5.158\n",
            "Epoch 2 Batch 1701 0.8853 0.9933 0.676 loss = 5.141\n",
            "Epoch 2 Batch 1801 0.8773 0.9907 0.652 loss = 5.097\n",
            "Epoch 2 Batch 1901 0.8933 0.9933 0.704 loss = 4.862\n",
            "Epoch 2 Batch 2001 0.892 0.9947 0.68 loss = 4.939\n",
            "Epoch 2 Batch 2101 0.9067 0.9867 0.728 loss = 4.93\n",
            "Epoch 2 Batch 2201 0.8987 0.9827 0.72 loss = 4.967\n",
            "Epoch 2 Batch 2301 0.9093 0.9947 0.74 loss = 4.833\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}