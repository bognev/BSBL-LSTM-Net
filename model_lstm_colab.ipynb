{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_lstm_colab",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bognev/BSBL-LSTM-Net/blob/master/model_lstm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baABRS9GKVnX",
        "colab_type": "code",
        "outputId": "36c03847-8c03-417b-f7ca-e152fba31f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13198
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from mat4py import loadmat\n",
        "# #from torchsummary import summary\n",
        "# from graphviz import Digraph\n",
        "# from torchviz import make_dot\n",
        "# from graphviz import Source\n",
        "\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BuildLstmStack(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, rnn_size, num_layers):\n",
        "        super(BuildLstmStack, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.rnn_size = rnn_size\n",
        "        self.num_layers = num_layers\n",
        "        self.all_layers = []\n",
        "        l_i2h_lst = [nn.Linear(self.input_size, 4 * self.rnn_size)]\n",
        "        l_h2h_lst = [nn.Linear(self.rnn_size, 4 * self.rnn_size)]\n",
        "        l_bn_lst = [nn.BatchNorm1d(4 * self.rnn_size)]\n",
        "        for L in range(1, self.num_layers):\n",
        "            l_i2h_lst.append(nn.Linear(self.rnn_size, 4 * self.rnn_size))\n",
        "            l_h2h_lst.append(nn.Linear(self.rnn_size, 4 * self.rnn_size))\n",
        "            l_bn_lst.append(nn.BatchNorm1d(4 * self.rnn_size))\n",
        "        self.l_i2h = nn.ModuleList(l_i2h_lst)\n",
        "        self.l_h2h = nn.ModuleList(l_h2h_lst)\n",
        "        self.l_bn = nn.ModuleList(l_bn_lst)\n",
        "\n",
        "    def forward(self, x, prev_hs, prev_cs):\n",
        "        self.x_size = []\n",
        "        self.prev_c = 0\n",
        "        self.prev_h = 0\n",
        "        self.next_hs = []\n",
        "        self.next_cs = []\n",
        "        self.i2h = []\n",
        "        self.h2h = []\n",
        "        for L in range(self.num_layers):\n",
        "            self.prev_c = prev_cs[L]\n",
        "            self.prev_h = prev_hs[L]\n",
        "            if L == 0:\n",
        "                self.x = x\n",
        "            else:\n",
        "                self.x = self.next_hs[L - 1]\n",
        "            self.i2h.append(self.l_i2h[L](self.x))\n",
        "            self.h2h.append(self.l_h2h[L](self.prev_h))\n",
        "            all_sums = self.l_bn[L](self.i2h[L] + self.h2h[L])\n",
        "            (n1, n2, n3, n4) = all_sums.chunk(4, dim=1)  # it should return 4 tensors self.rnn_size\n",
        "            in_gate = torch.sigmoid(n1)\n",
        "            forget_gate = torch.sigmoid(n2)\n",
        "            out_gate = torch.sigmoid(n3)\n",
        "            in_transform = torch.tanh(n4)\n",
        "            next_c = forget_gate * self.prev_c + in_gate * in_transform\n",
        "            next_h = out_gate * torch.tanh(next_c)\n",
        "\n",
        "            self.next_hs.append(next_h)\n",
        "            self.next_cs.append(next_c)\n",
        "        return torch.stack(self.next_hs), torch.stack(self.next_cs)  # , i2h, h2h\n",
        "\n",
        "\n",
        "class BuildLstmUnrollNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_unroll, num_layers, rnn_size, input_size):\n",
        "        super(BuildLstmUnrollNet, self).__init__()\n",
        "        self.num_unroll = num_unroll\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_size = rnn_size\n",
        "        self.input_size = input_size\n",
        "        self.outputs = []\n",
        "        self.output = []\n",
        "        self.now_h, self.now_c = [], []\n",
        "        self.buildlstmstack_lst = []\n",
        "        for i in range(0, self.num_unroll):\n",
        "            self.buildlstmstack_lst.append(BuildLstmStack(self.input_size, self.rnn_size, self.num_layers))\n",
        "        self.buildlstmstack = nn.ModuleList(self.buildlstmstack_lst)\n",
        "\n",
        "    def forward(self, x, init_states_input):\n",
        "\n",
        "        self.init_hs = []\n",
        "        self.init_cs = []\n",
        "        self.now_hs = []\n",
        "        self.now_cs = []\n",
        "        self.outputs = []\n",
        "\n",
        "        init_states = init_states_input.reshape((init_states_input.size(0), self.num_layers * 2, self.rnn_size))\n",
        "        init_states_lst = list(init_states.chunk(self.num_layers * 2, 1))\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            self.init_hs.append(init_states_lst[2 * i].reshape(init_states_input.size(0), self.rnn_size))\n",
        "            self.init_cs.append(init_states_lst[2 * i + 1].reshape(init_states_input.size(0), self.rnn_size))\n",
        "\n",
        "        self.now_hs.append(torch.stack(self.init_hs))\n",
        "        self.now_cs.append(torch.stack(self.init_cs))\n",
        "\n",
        "        for i in range(self.num_unroll):\n",
        "            self.now_h, self.now_c = self.buildlstmstack[i](x, self.now_hs[i], self.now_cs[i])\n",
        "            self.now_hs.append(self.now_h)\n",
        "            self.now_cs.append(self.now_c)\n",
        "            # self.outputs.append(torch.cat(self.now_hs[-1],1))\n",
        "            self.outputs.append(self.now_hs[i + 1][-1])\n",
        "            # for L in range(self.num_layers):\n",
        "            #     setattr(self, 'hid_%d_%d' %(i, L), self.now_hs[i][L])\n",
        "            #     setattr(self, 'cell_%d_%d' %(i, L), self.now_cs[i][L])\n",
        "        for i in range(1, self.num_unroll):\n",
        "            for j in range(self.num_layers):\n",
        "                self.buildlstmstack[i].l_i2h[j].weight.data = self.buildlstmstack[0].l_i2h[j].weight.data\n",
        "                self.buildlstmstack[i].l_h2h[j].weight.data = self.buildlstmstack[0].l_h2h[j].weight.data\n",
        "                self.buildlstmstack[i].l_i2h[j].bias.data = self.buildlstmstack[0].l_i2h[j].bias.data\n",
        "                self.buildlstmstack[i].l_h2h[j].bias.data = self.buildlstmstack[0].l_h2h[j].bias.data\n",
        "                self.buildlstmstack[i].l_i2h[j].weight.grad = self.buildlstmstack[0].l_i2h[j].weight.grad\n",
        "                self.buildlstmstack[i].l_h2h[j].weight.grad = self.buildlstmstack[0].l_h2h[j].weight.grad\n",
        "                self.buildlstmstack[i].l_i2h[j].bias.grad = self.buildlstmstack[0].l_i2h[j].bias.grad\n",
        "                self.buildlstmstack[i].l_h2h[j].bias.grad = self.buildlstmstack[0].l_h2h[j].bias.grad\n",
        "        self.output = self.outputs[0]\n",
        "        for i in range(1, self.num_unroll):\n",
        "            self.output = torch.cat((self.output, self.outputs[i]), 1)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class GetLstmNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_unroll, num_layers, rnn_size, output_size, input_size):\n",
        "        super(GetLstmNet, self).__init__()\n",
        "        self.num_unroll, self.num_layers, self.rnn_size, self.output_size, self.input_size = num_unroll, num_layers, rnn_size, output_size, input_size\n",
        "        self.l_pred_l = nn.Linear(self.num_unroll * self.rnn_size, self.output_size)\n",
        "        self.lstmnet = BuildLstmUnrollNet(self.num_unroll, self.num_layers, self.rnn_size, self.input_size)\n",
        "        self.l_pred_bn = nn.BatchNorm1d(self.output_size)\n",
        "        # setattr(self, 'LstmNetLinear', self.l_pred_l)\n",
        "\n",
        "    def forward(self, x, init_states_input):\n",
        "        self.lstm_output = self.lstmnet(x, init_states_input)\n",
        "        self.pred = self.l_pred_bn(self.l_pred_l(self.lstm_output))\n",
        "        return self.pred\n",
        "\n",
        "\n",
        "###########Usage#######################################\n",
        "\n",
        "input_size = 20\n",
        "output_size = 50\n",
        "rnn_size = 10\n",
        "num_layers = 2\n",
        "num_unroll = 3\n",
        "# graph of net\n",
        "x = torch.rand(3, input_size)\n",
        "z = torch.zeros(3, rnn_size * num_layers * 2)\n",
        "\n",
        "\n",
        "# model = BuildLstmStack(input_size, rnn_size, num_layers)\n",
        "# init_hs = []\n",
        "# init_cs = []\n",
        "# init_states = z.reshape((z.size(0),num_layers * 2, rnn_size))\n",
        "# init_states_lst = list(init_states.chunk(num_layers * 2,1))\n",
        "# for i in range(num_layers):\n",
        "#     init_hs.append(init_states_lst[2*i].reshape(num_layers,rnn_size))\n",
        "#     init_cs.append(init_states_lst[2*i+1].reshape(num_layers,rnn_size))\n",
        "# now_hs, now_cs = model(x, init_hs, init_cs)\n",
        "# temp = make_dot((now_hs[2], now_cs[2]), params=dict(list(model.named_parameters())))\n",
        "# s = Source(temp, filename=\"BuildLstmStack.gv\", format=\"png\")\n",
        "# s.view()\n",
        "#\n",
        "# model = BuildLstmUnrollNet(num_unroll, num_layers, rnn_size, input_size)\n",
        "# out = model(x, z)\n",
        "# temp = make_dot(out, params=dict(list(model.named_parameters())+ [('x', x)]+ [('z', z)]))\n",
        "# s = Source(temp, filename=\"BuildLstmUnrollNet.gv\", format=\"png\")\n",
        "# s.view()\n",
        "#\n",
        "# model = GetLstmNet(num_unroll, num_layers, rnn_size, output_size, input_size)\n",
        "# output = model(x,z)\n",
        "# for i in range(1, num_unroll):\n",
        "#     for j in range(num_layers):\n",
        "#         model.lstmnet.buildlstmstack[i].l_i2h[j].weight = model.lstmnet.buildlstmstack[0].l_i2h[j].weight\n",
        "#         model.lstmnet.buildlstmstack[i].l_h2h[j].weight = model.lstmnet.buildlstmstack[0].l_h2h[j].weight\n",
        "#         model.lstmnet.buildlstmstack[i].l_i2h[j].bias = model.lstmnet.buildlstmstack[0].l_i2h[j].bias\n",
        "#         model.lstmnet.buildlstmstack[i].l_h2h[j].bias = model.lstmnet.buildlstmstack[0].l_h2h[j].bias\n",
        "# print(model)\n",
        "# temp = make_dot(output, params=dict(list(model.named_parameters())+ [('x', x)]+ [('z', z)]))\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "# modell = nn.Sequential()\n",
        "# modell.add_module('W0', nn.Linear(8, 16))\n",
        "# modell.add_module('tanh', nn.Tanh())\n",
        "# modell.add_module('W1', nn.Linear(16, 1))\n",
        "#\n",
        "# x = torch.randn(1,8)\n",
        "#\n",
        "# temp = make_dot(modell(x), params=dict(modell.named_parameters()))\n",
        "#\n",
        "# s = Source(temp, filename=\"test.gv\", format=\"png\")\n",
        "# s.view()\n",
        "\n",
        "class MultiClassNLLCriterion(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MultiClassNLLCriterion, self).__init__()\n",
        "        self.lsm = nn.LogSoftmax(dim=1)\n",
        "        self.nll = nn.NLLLoss()\n",
        "        self.output = 0\n",
        "        self.outputs = 0\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        self.output = self.lsm(inputs)\n",
        "        shape = target.shape\n",
        "        self.outputs = 0\n",
        "        # print(self.output.shape)\n",
        "        # print(target.shape)\n",
        "        for i in range(0, shape[1]):\n",
        "            self.outputs = self.outputs + self.nll(self.output, target[:, i].squeeze())\n",
        "        return self.outputs  # /shape[1]\n",
        "\n",
        "\n",
        "# match number\n",
        "def AccS(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(num_nonz)  # ?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).to(device)\n",
        "    #     print(label.get_device())\n",
        "    #     print(pred.get_device())\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, num_nonz):\n",
        "            t_score[:, i].add_(label[:, i].float().eq(pred[:, j]).float())\n",
        "    return t_score.mean()\n",
        "\n",
        "\n",
        "# loose match\n",
        "def AccL(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(20)  # ?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).to(device)\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, 20):\n",
        "            t_score[:, i].add_(\n",
        "                label[:, i].float().eq(pred[:, j]).float())  # t_score[:,i].add(label[:,i].eq(pred[:,j])).float()\n",
        "    return t_score.mean()\n",
        "\n",
        "\n",
        "# sctrict match\n",
        "def AccM(label, pred_prob):\n",
        "    num_nonz = label.shape[1]\n",
        "    _, pred = pred_prob.topk(num_nonz)  # ?!\n",
        "    pred = pred.float()\n",
        "    t_score = torch.zeros(label.shape).to(device)\n",
        "    for i in range(0, num_nonz):\n",
        "        for j in range(0, num_nonz):\n",
        "            t_score[:, i].add_(\n",
        "                label[:, i].float().eq(pred[:, j]).float())  # t_score[:,i].add(label[:,i].eq(pred[:,j])).float()\n",
        "    return t_score.sum(1).eq(num_nonz).sum().item() * 1. / pred.shape[0]\n",
        "\n",
        "\n",
        "gpu = 1  # gpu id\n",
        "batch_size = 250  # 10# training batch size\n",
        "lr = 0.002  # basic learning rate\n",
        "lr_decay_startpoint = 250  # learning rate from which epoch\n",
        "num_epochs = 400  # total training epochs\n",
        "max_grad_norm = 5.0\n",
        "clip_gradient = 4.0\n",
        "\n",
        "# task related parameters\n",
        "# task: y = Ax, given A recovery sparse x from y\n",
        "dataset = 'uniform'  # type of non-zero elements: uniform ([-1,-0.1]U[0.1,1]), unit (+-1)\n",
        "num_nonz = 3  # number of non-zero elemetns to recovery: 3,4,5,6,7,8,9,10\n",
        "input_size = 20  # dimension of observation vector y\n",
        "output_size = 100  # dimension of sparse vector x\n",
        "\n",
        "# model hyper parameters\n",
        "rnn_size = 425  # number of units in RNN cell\n",
        "num_layers = 2  # number of stacked RNN layers\n",
        "num_unroll = 11  # number of RNN unrolled time steps\n",
        "\n",
        "# torch.set_num_threads(16)\n",
        "# manualSeed = torch.randint(1,10000,(1,))\n",
        "# print(\"Random seed \" + str(manualSeed.item()))\n",
        "torch.set_default_tensor_type(torch.FloatTensor)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_size = 600000  # 100\n",
        "valid_size = 100000  # 10#\n",
        "valid_data = torch.zeros(valid_size, input_size).to(device)\n",
        "valid_label = torch.zeros(valid_size, num_nonz).type(torch.LongTensor).to(device)\n",
        "batch_data = torch.zeros(batch_size, input_size).to(device)\n",
        "batch_label = torch.zeros(batch_size, num_nonz).to(device)  # for MultiClassNLLCriterion LOSS\n",
        "batch_zero_states = torch.zeros(batch_size, num_layers * rnn_size * 2).to(device)  # init_states for lstm\n",
        "\n",
        "# AccM, AccL, Accs = 0, 0, 0\n",
        "\n",
        "\n",
        "err = 0\n",
        "\n",
        "model_all = \"model_l_\" + str(num_layers) + \"t_\" + str(num_unroll) + '_rnn_' + str(rnn_size)\n",
        "logger_file = model_all + str(dataset) + \"_\" + str(num_nonz) + '.log'\n",
        "logger = open(logger_file, 'w')\n",
        "# for k,v in pairs(opt) do logger:write(k .. ' ' .. v ..'\\n') end\n",
        "# logger:write('network have ' .. paras:size(1) .. ' parameters' .. '\\n')\n",
        "# logger:close()\n",
        "\n",
        "# torch.manual_seed(10)\n",
        "# mat_A = torch.rand(output_size,input_size)\n",
        "mat_A = torch.load(\"/content/gdrive/My Drive/mat_A.pt\").to(device)\n",
        "\n",
        "\n",
        "def gen_batch(batch_size, num_nonz, mat_A):\n",
        "    # mat_A = loadmat('matrix_corr_unit_20_100.mat')\n",
        "    # mat_A = torch.FloatTensor(mat_A['A']).t()\n",
        "    # print(mat_A.shape)\n",
        "    # mat_A = torch.rand(output_size, input_size)\n",
        "    batch_X = torch.Tensor(batch_size, 100).to(device)\n",
        "    batch_n = torch.Tensor(batch_size, num_nonz).to(device)\n",
        "    bs = batch_size\n",
        "    len = int(100 / num_nonz * num_nonz)\n",
        "    perm = torch.randperm(100)[range(len)].to(device)\n",
        "    #     batch_label = torch.zeros(batch_size, num_nonz).type(torch.LongTensor).to(device)  # for MultiClassNLLCriterion LOSS\n",
        "    for i in range(int(bs * num_nonz / len)):\n",
        "        perm = torch.cat((perm, torch.randperm(100)[range(len)].to(device)))\n",
        "    batch_label = perm[range(bs * num_nonz)].reshape([bs, num_nonz]).type(torch.LongTensor).to(device)\n",
        "    batch_X.zero_()\n",
        "    if dataset == 'uniform':\n",
        "        batch_n.uniform_(-0.5, 0.5)\n",
        "        batch_n[batch_n.gt(0)] = batch_n[batch_n.gt(0)] + 0.1\n",
        "        batch_n[batch_n.le(0)] = batch_n[batch_n.le(0)] - 0.1\n",
        "    #\n",
        "    # print(batch_X.shape)\n",
        "    #     print(batch_X.get_device())\n",
        "    #     print(mat_A.get_device())\n",
        "    #     print(batch_n.get_device())\n",
        "    for i in range(bs):\n",
        "        for j in range(num_nonz):\n",
        "            batch_X[i][batch_label[i][j]] = batch_n[i][j]\n",
        "    batch_data = torch.mm(batch_X, mat_A)  # +0.001*torch.randn(batch_size,input_size).to(device)\n",
        "    # print(batch_label.shape)\n",
        "    # print(batch_data.shape)\n",
        "    return batch_label, batch_data\n",
        "\n",
        "\n",
        "print(\"building validation set\")\n",
        "for i in range(0, valid_size, batch_size):\n",
        "    #     mat_A = torch.rand(output_size, input_size).to(device)\n",
        "    batch_label, batch_data = gen_batch(batch_size, num_nonz, mat_A)\n",
        "    # print(batch_label.shape)\n",
        "    # print(\"batch_data shape = \" + str(batch_data.shape))\n",
        "    # print(\"valid_data shape = \" + str(valid_data.shape))\n",
        "    # print(range(i,i+batch_size-1))\n",
        "    valid_data[range(i, i + batch_size), :] = batch_data\n",
        "    valid_label[range(i, i + batch_size), :] = batch_label\n",
        "print('done')\n",
        "\n",
        "best_valid_accs = 0\n",
        "base_epoch = lr_decay_startpoint\n",
        "base_lr = lr\n",
        "optimState = {'learningRate': 0.001, 'weigthDecay': 0.0000}\n",
        "\n",
        "net = GetLstmNet(num_unroll, num_layers, rnn_size, output_size, input_size)\n",
        "# print(net)\n",
        "device = torch.device('cuda')\n",
        "net.to(device)\n",
        "# summary(net,[(num_layers,input_size),(num_layers,rnn_size * num_layers * 2)])\n",
        "# summary(net,[(batch_size, input_size),(batch_size, num_layers * rnn_size * 2)])\n",
        "\n",
        "# create a stochastic gradient descent optimizer\n",
        "# optimizer = optim.RMSprop(params=net.parameters(), lr=0.001, alpha=0.9, eps=1e-04, weight_decay=0.0001, momentum=0, centered=False)\n",
        "# create a loss function\n",
        "LOSS = MultiClassNLLCriterion()\n",
        "optimizer = optim.RMSprop(params=net.parameters(), lr=optimState['learningRate'], \\\n",
        "                          alpha=0.99, eps=1e-05, weight_decay=optimState['weigthDecay'], momentum=0.0, centered=False)\n",
        "\n",
        "# checkpoint = torch.load( \"/content/gdrive/My Drive/model_l_2t_17_rnn_800_3.pth\")\n",
        "# net.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# epoch = checkpoint['epoch'] + 1\n",
        "# loss = checkpoint['loss']\n",
        "epoch = 0\n",
        "\n",
        "# mat_A = torch.rand(output_size, input_size).to(device)\n",
        "for epoch in range(epoch, num_epochs):\n",
        "\n",
        "    # learing rate self - adjustment\n",
        "    # if(epoch > 250):\n",
        "    #     optimState['learningRate'] = base_lr / (1 + 0.06 * (epoch - base_epoch))\n",
        "    #     if(epoch % 50 == 0): base_epoch = epoch; base_lr= base_lr * 0.25\n",
        "\n",
        "    logger = open(logger_file, 'a')\n",
        "    # train\n",
        "    train_accs = 0\n",
        "    train_accl = 0\n",
        "    train_accm = 0\n",
        "    train_err = 0\n",
        "    nbatch = 0\n",
        "\n",
        "    net.train()\n",
        "    start = time.time()\n",
        "    for i in range(0, train_size, batch_size):\n",
        "        batch_label, batch_data = gen_batch(batch_size, num_nonz, mat_A)\n",
        "        batch_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred_prob = net(batch_data, batch_zero_states).to(device)  # 0 or 1?!\n",
        "        err = LOSS(pred_prob, batch_label.to(device))\n",
        "        err.backward()\n",
        "        with torch.no_grad():\n",
        "            for name, param in net.named_parameters():\n",
        "                # print(name)\n",
        "                # print(param.grad.data)\n",
        "                param.grad.clamp_(-4.0, 4.0)\n",
        "                gnorm = param.grad.norm()\n",
        "                if (gnorm > max_grad_norm):\n",
        "                    param.grad.mul_(max_grad_norm / gnorm)\n",
        "        optimizer.step()\n",
        "        #         print(pred_prob.get_device())\n",
        "        #         print(batch_label.get_device())\n",
        "        batch_accs = AccS(batch_label[:, range(0, num_nonz)], pred_prob.to(device).float())\n",
        "        batch_accl = AccL(batch_label[:, range(0, num_nonz)], pred_prob.to(device).float())\n",
        "        batch_accm = AccM(batch_label[:, range(0, num_nonz)], pred_prob.to(device).float())\n",
        "        train_accs = train_accs + batch_accs.item()\n",
        "        train_accl = train_accl + batch_accl.item()\n",
        "        train_accm = train_accm + batch_accm\n",
        "        train_err = train_err + err.item()\n",
        "        nbatch = nbatch + 1\n",
        "        if (nbatch) % 512 == 1:\n",
        "            print(\"Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" {:.4} {:.4} {:.4} loss = {:.4}\".format(batch_accs,\n",
        "                                                                                                             batch_accl,\n",
        "                                                                                                             batch_accm,\n",
        "                                                                                                             err.item()))\n",
        "    end = time.time()\n",
        "    print(\"Train [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\".format(epoch, end - start, \\\n",
        "                                                                                    train_accs / nbatch,\n",
        "                                                                                    train_accl / nbatch, \\\n",
        "                                                                                    train_accm / nbatch,\n",
        "                                                                                    train_err / nbatch))\n",
        "    logger.write(\"Train [{}] Time {:.4} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\\n\".format(epoch, end - start, \\\n",
        "                                                                                                train_accs / nbatch,\n",
        "                                                                                                train_accl / nbatch, \\\n",
        "                                                                                                train_accm / nbatch,\n",
        "                                                                                                train_err / nbatch))\n",
        "\n",
        "    # eval\n",
        "    nbatch = 0\n",
        "    valid_accs = 0\n",
        "    valid_accl = 0\n",
        "    valid_accm = 0\n",
        "    valid_err = 0\n",
        "    start = time.time()\n",
        "    net.eval()\n",
        "    for i in range(0, valid_size, batch_size):\n",
        "        batch_data = valid_data[range(i, i + batch_size), :]\n",
        "        batch_label[:, range(0, num_nonz)] = valid_label[range(i, i + batch_size), :]\n",
        "        pred_prob = net(batch_data, batch_zero_states)\n",
        "        err = LOSS(pred_prob, batch_label)\n",
        "        batch_accs = AccS(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        batch_accl = AccL(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        batch_accm = AccM(batch_label[:, range(0, num_nonz)], pred_prob.float())\n",
        "        valid_accs = valid_accs + batch_accs.item()\n",
        "        valid_accl = valid_accl + batch_accl.item()\n",
        "        valid_accm = valid_accm + batch_accm\n",
        "        valid_err = valid_err + err.item()\n",
        "        nbatch = nbatch + 1\n",
        "    #         if (nbatch+99) % 100 == 0:\n",
        "    #             print(\"Eval Epoch \" + str(epoch) + \" Batch \" + str(nbatch) + \" {:.4} {:.4} {:.4} loss = {:.4}\".format(batch_accs, batch_accl,\n",
        "    #                                                                                             batch_accm, err.item()))\n",
        "    end = time.time()\n",
        "    print(\"Valid [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\".format(epoch, end - start, \\\n",
        "                                                                                    valid_accs / nbatch,\n",
        "                                                                                    valid_accl / nbatch, \\\n",
        "                                                                                    valid_accm / nbatch,\n",
        "                                                                                    valid_err / nbatch))\n",
        "    logger.write(\"Valid [{}] Time {} s-acc {:.4} l-acc {:.4} m-acc {:.4} err {:.4}\\n\".format(epoch, end - start, \\\n",
        "                                                                                             valid_accs / nbatch,\n",
        "                                                                                             valid_accl / nbatch, \\\n",
        "                                                                                             train_accm / nbatch,\n",
        "                                                                                             valid_err / nbatch))\n",
        "    # if(valid_accs > best_valid_accs):\n",
        "    #     best_valid_accs = valid_accs\n",
        "    #     print(\"saving model\")\n",
        "    #     logger.write('saving model\\n')\n",
        "    #     checkpoint = {'epoch': epoch,\n",
        "    #                   'model_state_dict': net.state_dict(),\n",
        "    #                   'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #                   'loss': err.item()}\n",
        "    #     # torch.save(checkpoint, 'checkpoint.pth')\n",
        "    #     torch.save(checkpoint, \"./checkpoints/\"+model_all+\"_\"+str(num_nonz)+\".pth\") #or torch.save(net, PATH)\n",
        "    #     #net.load_state_dict(torch.load(PATH)) # or the_model = torch.load(PATH)\n",
        "\n",
        "    # if(epoch % 2 == 0):\n",
        "    print(\"saving model\")\n",
        "    logger.write('saving model\\n')\n",
        "    checkpoint = {'epoch': epoch, \\\n",
        "                  'model_state_dict': net.state_dict(), \\\n",
        "                  'optimizer_state_dict': optimizer.state_dict(), \\\n",
        "                  'loss': err.item()}\n",
        "    torch.save(checkpoint,\n",
        "               \"/content/gdrive/My Drive/\" + model_all + \"_\" + str(num_nonz) + \".pth\")  # or torch.save(net, PATH)\n",
        "    logger.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "building validation set\n",
            "done\n",
            "Epoch 0 Batch 1 0.03067 0.1973 0.0 loss = 14.28\n",
            "Epoch 0 Batch 513 0.3453 0.6453 0.0 loss = 11.07\n",
            "Epoch 0 Batch 1025 0.444 0.7387 0.024 loss = 9.967\n",
            "Epoch 0 Batch 1537 0.5053 0.804 0.056 loss = 9.204\n",
            "Epoch 0 Batch 2049 0.5467 0.8307 0.076 loss = 8.749\n",
            "Train [0] Time 185.46652388572693 s-acc 0.446 l-acc 0.727 m-acc 0.03486 err 9.985\n",
            "Valid [0] Time 7.572957992553711 s-acc 0.5484 l-acc 0.8365 m-acc 0.07678 err 8.717\n",
            "saving model\n",
            "Epoch 1 Batch 1 0.6173 0.8587 0.116 loss = 8.116\n",
            "Epoch 1 Batch 513 0.6387 0.8947 0.12 loss = 7.782\n",
            "Epoch 1 Batch 1025 0.6373 0.9173 0.156 loss = 7.56\n",
            "Epoch 1 Batch 1537 0.644 0.8947 0.156 loss = 7.609\n",
            "Epoch 1 Batch 2049 0.672 0.9107 0.192 loss = 7.397\n",
            "Train [1] Time 184.7507345676422 s-acc 0.6466 l-acc 0.9002 m-acc 0.1714 err 7.604\n",
            "Valid [1] Time 6.7143471240997314 s-acc 0.648 l-acc 0.905 m-acc 0.1936 err 7.504\n",
            "saving model\n",
            "Epoch 2 Batch 1 0.7133 0.9333 0.296 loss = 6.933\n",
            "Epoch 2 Batch 513 0.736 0.9307 0.312 loss = 6.666\n",
            "Epoch 2 Batch 1025 0.716 0.9453 0.288 loss = 6.743\n",
            "Epoch 2 Batch 1537 0.7333 0.9453 0.324 loss = 6.659\n",
            "Epoch 2 Batch 2049 0.7467 0.9627 0.372 loss = 6.39\n",
            "Train [2] Time 185.5271065235138 s-acc 0.7252 l-acc 0.9442 m-acc 0.3126 err 6.662\n",
            "Valid [2] Time 6.720165967941284 s-acc 0.7377 l-acc 0.9521 m-acc 0.3485 err 6.44\n",
            "saving model\n",
            "Epoch 3 Batch 1 0.7627 0.9667 0.396 loss = 6.245\n",
            "Epoch 3 Batch 513 0.7707 0.9613 0.4 loss = 6.184\n",
            "Epoch 3 Batch 1025 0.776 0.952 0.408 loss = 6.094\n",
            "Epoch 3 Batch 1537 0.788 0.9747 0.44 loss = 5.847\n",
            "Epoch 3 Batch 2049 0.816 0.98 0.532 loss = 5.701\n",
            "Train [3] Time 186.2470805644989 s-acc 0.7766 l-acc 0.9645 m-acc 0.4282 err 6.062\n",
            "Valid [3] Time 6.743931770324707 s-acc 0.7438 l-acc 0.9513 m-acc 0.3868 err 6.371\n",
            "saving model\n",
            "Epoch 4 Batch 1 0.788 0.9733 0.448 loss = 5.954\n",
            "Epoch 4 Batch 513 0.8187 0.9773 0.548 loss = 5.567\n",
            "Epoch 4 Batch 1025 0.8053 0.968 0.516 loss = 5.849\n",
            "Epoch 4 Batch 1537 0.8307 0.9867 0.556 loss = 5.408\n",
            "Epoch 4 Batch 2049 0.8307 0.984 0.544 loss = 5.414\n",
            "Train [4] Time 184.41426253318787 s-acc 0.8176 l-acc 0.9764 m-acc 0.525 err 5.601\n",
            "Valid [4] Time 6.714384317398071 s-acc 0.8116 l-acc 0.9745 m-acc 0.5221 err 5.604\n",
            "saving model\n",
            "Epoch 5 Batch 1 0.8427 0.98 0.588 loss = 5.35\n",
            "Epoch 5 Batch 513 0.832 0.9853 0.564 loss = 5.281\n",
            "Epoch 5 Batch 1025 0.844 0.9827 0.592 loss = 5.351\n",
            "Epoch 5 Batch 1537 0.864 0.9827 0.628 loss = 5.17\n",
            "Epoch 5 Batch 2049 0.8347 0.9853 0.584 loss = 5.324\n",
            "Train [5] Time 185.28572940826416 s-acc 0.8484 l-acc 0.9834 m-acc 0.6019 err 5.256\n",
            "Valid [5] Time 7.121777296066284 s-acc 0.8292 l-acc 0.9784 m-acc 0.5657 err 5.394\n",
            "saving model\n",
            "Epoch 6 Batch 1 0.8653 0.984 0.66 loss = 5.088\n",
            "Epoch 6 Batch 513 0.856 0.9827 0.628 loss = 5.145\n",
            "Epoch 6 Batch 1025 0.828 0.9773 0.564 loss = 5.413\n",
            "Epoch 6 Batch 1537 0.88 0.9907 0.68 loss = 4.912\n",
            "Epoch 6 Batch 2049 0.904 0.9973 0.74 loss = 4.584\n",
            "Train [6] Time 185.15664291381836 s-acc 0.8735 l-acc 0.9881 m-acc 0.6646 err 4.977\n",
            "Valid [6] Time 7.215938568115234 s-acc 0.8491 l-acc 0.9809 m-acc 0.6145 err 5.189\n",
            "saving model\n",
            "Epoch 7 Batch 1 0.8707 0.9867 0.66 loss = 5.019\n",
            "Epoch 7 Batch 513 0.8933 0.9947 0.72 loss = 4.706\n",
            "Epoch 7 Batch 1025 0.88 0.9947 0.68 loss = 4.831\n",
            "Epoch 7 Batch 1537 0.8693 0.9853 0.668 loss = 4.961\n",
            "Epoch 7 Batch 2049 0.9373 0.9973 0.812 loss = 4.402\n",
            "Train [7] Time 184.11186623573303 s-acc 0.8916 l-acc 0.9908 m-acc 0.7119 err 4.77\n",
            "Valid [7] Time 6.715692520141602 s-acc 0.8425 l-acc 0.978 m-acc 0.6154 err 5.248\n",
            "saving model\n",
            "Epoch 8 Batch 1 0.912 0.9947 0.76 loss = 4.513\n",
            "Epoch 8 Batch 513 0.9013 0.996 0.724 loss = 4.584\n",
            "Epoch 8 Batch 1025 0.9027 0.988 0.748 loss = 4.643\n",
            "Epoch 8 Batch 1537 0.9173 0.996 0.784 loss = 4.401\n",
            "Epoch 8 Batch 2049 0.8973 0.9933 0.736 loss = 4.631\n",
            "Train [8] Time 185.61415576934814 s-acc 0.906 l-acc 0.9928 m-acc 0.7498 err 4.601\n",
            "Valid [8] Time 6.6806700229644775 s-acc 0.8718 l-acc 0.9863 m-acc 0.6743 err 4.894\n",
            "saving model\n",
            "Epoch 9 Batch 1 0.908 0.9933 0.752 loss = 4.56\n",
            "Epoch 9 Batch 513 0.9253 0.9973 0.8 loss = 4.354\n",
            "Epoch 9 Batch 1025 0.9213 0.992 0.78 loss = 4.47\n",
            "Epoch 9 Batch 1537 0.916 0.9947 0.78 loss = 4.541\n",
            "Epoch 9 Batch 2049 0.9253 0.996 0.792 loss = 4.412\n",
            "Train [9] Time 185.45550966262817 s-acc 0.9168 l-acc 0.994 m-acc 0.7784 err 4.472\n",
            "Valid [9] Time 6.719430685043335 s-acc 0.9153 l-acc 0.9938 m-acc 0.7777 err 4.409\n",
            "saving model\n",
            "Epoch 10 Batch 1 0.9227 0.996 0.784 loss = 4.326\n",
            "Epoch 10 Batch 513 0.9187 0.9947 0.792 loss = 4.385\n",
            "Epoch 10 Batch 1025 0.9187 0.996 0.772 loss = 4.359\n",
            "Epoch 10 Batch 1537 0.936 0.9907 0.824 loss = 4.363\n",
            "Epoch 10 Batch 2049 0.9427 0.988 0.852 loss = 4.311\n",
            "Train [10] Time 184.18966555595398 s-acc 0.9252 l-acc 0.9949 m-acc 0.8012 err 4.368\n",
            "Valid [10] Time 6.724916696548462 s-acc 0.8638 l-acc 0.9818 m-acc 0.6717 err 4.988\n",
            "saving model\n",
            "Epoch 11 Batch 1 0.9133 0.9987 0.764 loss = 4.397\n",
            "Epoch 11 Batch 513 0.924 0.9987 0.808 loss = 4.375\n",
            "Epoch 11 Batch 1025 0.9227 0.9907 0.792 loss = 4.466\n",
            "Epoch 11 Batch 1537 0.948 1.0 0.86 loss = 4.077\n",
            "Epoch 11 Batch 2049 0.9467 0.9973 0.856 loss = 4.128\n",
            "Train [11] Time 187.26322436332703 s-acc 0.9312 l-acc 0.9954 m-acc 0.8176 err 4.292\n",
            "Valid [11] Time 6.693058013916016 s-acc 0.8475 l-acc 0.975 m-acc 0.6437 err 5.193\n",
            "saving model\n",
            "Epoch 12 Batch 1 0.9227 0.9907 0.788 loss = 4.323\n",
            "Epoch 12 Batch 513 0.9333 0.9973 0.824 loss = 4.262\n",
            "Epoch 12 Batch 1025 0.9333 0.9987 0.828 loss = 4.244\n",
            "Epoch 12 Batch 1537 0.948 0.9947 0.852 loss = 4.128\n",
            "Epoch 12 Batch 2049 0.9587 0.9987 0.892 loss = 4.033\n",
            "Train [12] Time 183.6243507862091 s-acc 0.9373 l-acc 0.9959 m-acc 0.834 err 4.216\n",
            "Valid [12] Time 6.654815912246704 s-acc 0.9344 l-acc 0.9955 m-acc 0.8301 err 4.178\n",
            "saving model\n",
            "Epoch 13 Batch 1 0.9667 1.0 0.904 loss = 3.949\n",
            "Epoch 13 Batch 513 0.9307 0.9933 0.836 loss = 4.275\n",
            "Epoch 13 Batch 1025 0.9493 0.996 0.864 loss = 4.153\n",
            "Epoch 13 Batch 1537 0.9547 1.0 0.868 loss = 4.064\n",
            "Epoch 13 Batch 2049 0.972 0.9973 0.928 loss = 3.967\n",
            "Train [13] Time 186.66144275665283 s-acc 0.9427 l-acc 0.9964 m-acc 0.8484 err 4.15\n",
            "Valid [13] Time 7.238128900527954 s-acc 0.9008 l-acc 0.9895 m-acc 0.7525 err 4.535\n",
            "saving model\n",
            "Epoch 14 Batch 1 0.9467 1.0 0.856 loss = 4.055\n",
            "Epoch 14 Batch 513 0.9093 0.9893 0.78 loss = 4.5\n",
            "Epoch 14 Batch 1025 0.9667 1.0 0.908 loss = 3.866\n",
            "Epoch 14 Batch 1537 0.9333 0.9907 0.832 loss = 4.212\n",
            "Epoch 14 Batch 2049 0.9493 0.9947 0.864 loss = 4.039\n",
            "Train [14] Time 184.19391083717346 s-acc 0.9461 l-acc 0.9967 m-acc 0.858 err 4.102\n",
            "Valid [14] Time 6.699267387390137 s-acc 0.9157 l-acc 0.9922 m-acc 0.7805 err 4.371\n",
            "saving model\n",
            "Epoch 15 Batch 1 0.9547 0.9973 0.876 loss = 4.0\n",
            "Epoch 15 Batch 513 0.9413 0.9987 0.836 loss = 4.122\n",
            "Epoch 15 Batch 1025 0.948 0.996 0.864 loss = 4.123\n",
            "Epoch 15 Batch 1537 0.9507 1.0 0.86 loss = 4.043\n",
            "Epoch 15 Batch 2049 0.9493 0.9947 0.86 loss = 4.14\n",
            "Train [15] Time 185.44149088859558 s-acc 0.9498 l-acc 0.997 m-acc 0.8676 err 4.055\n",
            "Valid [15] Time 6.674427032470703 s-acc 0.9499 l-acc 0.9969 m-acc 0.8706 err 3.989\n",
            "saving model\n",
            "Epoch 16 Batch 1 0.9667 1.0 0.908 loss = 3.842\n",
            "Epoch 16 Batch 513 0.9533 0.9973 0.876 loss = 4.054\n",
            "Epoch 16 Batch 1025 0.956 0.9947 0.896 loss = 4.023\n",
            "Epoch 16 Batch 1537 0.9573 0.9973 0.88 loss = 3.883\n",
            "Epoch 16 Batch 2049 0.9533 0.9947 0.888 loss = 4.008\n",
            "Train [16] Time 185.7168607711792 s-acc 0.953 l-acc 0.9972 m-acc 0.8766 err 4.013\n",
            "Valid [16] Time 6.709501266479492 s-acc 0.9448 l-acc 0.9963 m-acc 0.8586 err 4.031\n",
            "saving model\n",
            "Epoch 17 Batch 1 0.9587 0.9987 0.88 loss = 3.951\n",
            "Epoch 17 Batch 513 0.9573 0.9973 0.884 loss = 3.913\n",
            "Epoch 17 Batch 1025 0.9733 0.9973 0.932 loss = 3.872\n",
            "Epoch 17 Batch 1537 0.9547 0.996 0.892 loss = 4.038\n",
            "Epoch 17 Batch 2049 0.9427 0.9973 0.848 loss = 4.084\n",
            "Train [17] Time 183.91691207885742 s-acc 0.9552 l-acc 0.9972 m-acc 0.8827 err 3.983\n",
            "Valid [17] Time 6.721849679946899 s-acc 0.9454 l-acc 0.9964 m-acc 0.8596 err 4.014\n",
            "saving model\n",
            "Epoch 18 Batch 1 0.9627 1.0 0.896 loss = 3.898\n",
            "Epoch 18 Batch 513 0.9533 0.9973 0.88 loss = 4.029\n",
            "Epoch 18 Batch 1025 0.96 0.9973 0.888 loss = 3.963\n",
            "Epoch 18 Batch 1537 0.9347 0.9947 0.828 loss = 4.256\n",
            "Epoch 18 Batch 2049 0.952 0.9987 0.864 loss = 3.973\n",
            "Train [18] Time 185.23764419555664 s-acc 0.9575 l-acc 0.9975 m-acc 0.889 err 3.95\n",
            "Valid [18] Time 7.645906925201416 s-acc 0.9063 l-acc 0.9888 m-acc 0.7685 err 4.465\n",
            "saving model\n",
            "Epoch 19 Batch 1 0.9293 0.9947 0.836 loss = 4.223\n",
            "Epoch 19 Batch 513 0.94 0.996 0.836 loss = 4.036\n",
            "Epoch 19 Batch 1025 0.9627 1.0 0.9 loss = 3.913\n",
            "Epoch 19 Batch 1537 0.9507 0.996 0.864 loss = 4.031\n",
            "Epoch 19 Batch 2049 0.9747 1.0 0.924 loss = 3.818\n",
            "Train [19] Time 184.74434566497803 s-acc 0.9599 l-acc 0.9976 m-acc 0.895 err 3.92\n",
            "Valid [19] Time 6.760332345962524 s-acc 0.8678 l-acc 0.9772 m-acc 0.6932 err 4.969\n",
            "saving model\n",
            "Epoch 20 Batch 1 0.9427 1.0 0.832 loss = 4.004\n",
            "Epoch 20 Batch 513 0.9693 1.0 0.912 loss = 3.768\n",
            "Epoch 20 Batch 1025 0.968 0.996 0.928 loss = 3.827\n",
            "Epoch 20 Batch 1537 0.9547 0.9933 0.876 loss = 4.09\n",
            "Epoch 20 Batch 2049 0.96 0.9973 0.884 loss = 3.851\n",
            "Train [20] Time 184.12002539634705 s-acc 0.9618 l-acc 0.9977 m-acc 0.9003 err 3.894\n",
            "Valid [20] Time 6.648159503936768 s-acc 0.934 l-acc 0.9946 m-acc 0.83 err 4.13\n",
            "saving model\n",
            "Epoch 21 Batch 1 0.964 0.9987 0.912 loss = 3.839\n",
            "Epoch 21 Batch 513 0.964 0.9973 0.904 loss = 3.892\n",
            "Epoch 21 Batch 1025 0.964 0.9987 0.904 loss = 3.933\n",
            "Epoch 21 Batch 1537 0.9707 1.0 0.924 loss = 3.708\n",
            "Epoch 21 Batch 2049 0.9693 0.9973 0.924 loss = 3.845\n",
            "Train [21] Time 185.85087990760803 s-acc 0.9636 l-acc 0.9979 m-acc 0.9051 err 3.87\n",
            "Valid [21] Time 6.690789699554443 s-acc 0.9477 l-acc 0.9964 m-acc 0.8658 err 3.965\n",
            "saving model\n",
            "Epoch 22 Batch 1 0.9693 0.9973 0.92 loss = 3.822\n",
            "Epoch 22 Batch 513 0.9533 0.9987 0.864 loss = 3.974\n",
            "Epoch 22 Batch 1025 0.9507 0.9973 0.88 loss = 4.025\n",
            "Epoch 22 Batch 1537 0.9747 0.9987 0.932 loss = 3.706\n",
            "Epoch 22 Batch 2049 0.9547 0.9973 0.88 loss = 3.919\n",
            "Train [22] Time 184.27790570259094 s-acc 0.965 l-acc 0.998 m-acc 0.9087 err 3.85\n",
            "Valid [22] Time 6.749599933624268 s-acc 0.9466 l-acc 0.996 m-acc 0.8625 err 3.981\n",
            "saving model\n",
            "Epoch 23 Batch 1 0.96 0.996 0.888 loss = 3.893\n",
            "Epoch 23 Batch 513 0.9693 0.9973 0.932 loss = 3.816\n",
            "Epoch 23 Batch 1025 0.9747 0.9987 0.94 loss = 3.729\n",
            "Epoch 23 Batch 1537 0.9587 0.9973 0.896 loss = 3.831\n",
            "Epoch 23 Batch 2049 0.964 0.9987 0.908 loss = 3.795\n",
            "Train [23] Time 185.7467691898346 s-acc 0.9661 l-acc 0.998 m-acc 0.9119 err 3.833\n",
            "Valid [23] Time 7.305991888046265 s-acc 0.9104 l-acc 0.9878 m-acc 0.7764 err 4.432\n",
            "saving model\n",
            "Epoch 24 Batch 1 0.9493 0.996 0.86 loss = 3.98\n",
            "Epoch 24 Batch 513 0.9587 0.9933 0.912 loss = 3.917\n",
            "Epoch 24 Batch 1025 0.956 0.996 0.888 loss = 3.831\n",
            "Epoch 24 Batch 1537 0.9507 0.996 0.872 loss = 4.092\n",
            "Epoch 24 Batch 2049 0.9747 1.0 0.932 loss = 3.702\n",
            "Train [24] Time 186.05544447898865 s-acc 0.9674 l-acc 0.9981 m-acc 0.9154 err 3.813\n",
            "Valid [24] Time 6.981928110122681 s-acc 0.9399 l-acc 0.9948 m-acc 0.8457 err 4.053\n",
            "saving model\n",
            "Epoch 25 Batch 1 0.9707 0.996 0.924 loss = 3.788\n",
            "Epoch 25 Batch 513 0.972 0.9987 0.928 loss = 3.757\n",
            "Epoch 25 Batch 1025 0.9707 1.0 0.932 loss = 3.766\n",
            "Epoch 25 Batch 1537 0.972 0.9987 0.924 loss = 3.729\n",
            "Epoch 25 Batch 2049 0.9693 0.9933 0.924 loss = 3.86\n",
            "Train [25] Time 184.12334489822388 s-acc 0.9687 l-acc 0.9981 m-acc 0.919 err 3.798\n",
            "Valid [25] Time 6.704267978668213 s-acc 0.8778 l-acc 0.9769 m-acc 0.7227 err 4.85\n",
            "saving model\n",
            "Epoch 26 Batch 1 0.964 1.0 0.9 loss = 3.782\n",
            "Epoch 26 Batch 513 0.9747 1.0 0.924 loss = 3.735\n",
            "Epoch 26 Batch 1025 0.9613 0.9987 0.9 loss = 3.891\n",
            "Epoch 26 Batch 1537 0.98 1.0 0.944 loss = 3.703\n",
            "Epoch 26 Batch 2049 0.9707 0.9987 0.912 loss = 3.733\n",
            "Train [26] Time 186.93723368644714 s-acc 0.9694 l-acc 0.9982 m-acc 0.9209 err 3.786\n",
            "Valid [26] Time 6.747952461242676 s-acc 0.9303 l-acc 0.9925 m-acc 0.8264 err 4.164\n",
            "saving model\n",
            "Epoch 27 Batch 1 0.964 0.996 0.916 loss = 3.817\n",
            "Epoch 27 Batch 513 0.9747 0.9987 0.94 loss = 3.708\n",
            "Epoch 27 Batch 1025 0.9707 0.996 0.928 loss = 3.851\n",
            "Epoch 27 Batch 1537 0.9693 0.9973 0.924 loss = 3.821\n",
            "Epoch 27 Batch 2049 0.9653 1.0 0.908 loss = 3.744\n",
            "Train [27] Time 185.25233936309814 s-acc 0.9705 l-acc 0.9982 m-acc 0.9239 err 3.771\n",
            "Valid [27] Time 6.709219217300415 s-acc 0.9149 l-acc 0.9883 m-acc 0.7931 err 4.361\n",
            "saving model\n",
            "Epoch 28 Batch 1 0.976 1.0 0.936 loss = 3.684\n",
            "Epoch 28 Batch 513 0.968 0.9973 0.928 loss = 3.783\n",
            "Epoch 28 Batch 1025 0.972 0.9987 0.924 loss = 3.731\n",
            "Epoch 28 Batch 1537 0.976 0.9987 0.928 loss = 3.663\n",
            "Epoch 28 Batch 2049 0.972 1.0 0.924 loss = 3.694\n",
            "Train [28] Time 184.46575951576233 s-acc 0.9713 l-acc 0.9983 m-acc 0.9257 err 3.758\n",
            "Valid [28] Time 6.990312337875366 s-acc 0.8749 l-acc 0.9735 m-acc 0.7083 err 4.94\n",
            "saving model\n",
            "Epoch 29 Batch 1 0.9653 0.9987 0.904 loss = 3.795\n",
            "Epoch 29 Batch 513 0.9613 0.9987 0.904 loss = 3.859\n",
            "Epoch 29 Batch 1025 0.9773 0.9987 0.94 loss = 3.644\n",
            "Epoch 29 Batch 1537 0.9627 0.996 0.912 loss = 3.826\n",
            "Epoch 29 Batch 2049 0.9587 0.9973 0.9 loss = 3.896\n",
            "Train [29] Time 186.04476404190063 s-acc 0.9722 l-acc 0.9984 m-acc 0.9284 err 3.747\n",
            "Valid [29] Time 6.677683591842651 s-acc 0.9775 l-acc 0.9984 m-acc 0.9444 err 3.625\n",
            "saving model\n",
            "Epoch 30 Batch 1 0.98 1.0 0.948 loss = 3.639\n",
            "Epoch 30 Batch 513 0.968 0.9973 0.912 loss = 3.805\n",
            "Epoch 30 Batch 1025 0.9653 0.996 0.916 loss = 3.828\n",
            "Epoch 30 Batch 1537 0.9853 0.9973 0.96 loss = 3.697\n",
            "Epoch 30 Batch 2049 0.972 0.9987 0.932 loss = 3.732\n",
            "Train [30] Time 183.6041030883789 s-acc 0.973 l-acc 0.9984 m-acc 0.9305 err 3.737\n",
            "Valid [30] Time 6.686007022857666 s-acc 0.9259 l-acc 0.9917 m-acc 0.8169 err 4.218\n",
            "saving model\n",
            "Epoch 31 Batch 1 0.9773 0.9987 0.944 loss = 3.763\n",
            "Epoch 31 Batch 513 0.976 0.996 0.94 loss = 3.754\n",
            "Epoch 31 Batch 1025 0.9853 0.9987 0.964 loss = 3.615\n",
            "Epoch 31 Batch 1537 0.9733 1.0 0.92 loss = 3.694\n",
            "Epoch 31 Batch 2049 0.976 0.996 0.944 loss = 3.75\n",
            "Train [31] Time 186.53635001182556 s-acc 0.9737 l-acc 0.9984 m-acc 0.9327 err 3.725\n",
            "Valid [31] Time 6.7383949756622314 s-acc 0.9302 l-acc 0.9921 m-acc 0.8254 err 4.169\n",
            "saving model\n",
            "Epoch 32 Batch 1 0.9667 0.9987 0.908 loss = 3.783\n",
            "Epoch 32 Batch 513 0.9707 0.9973 0.936 loss = 3.753\n",
            "Epoch 32 Batch 1025 0.9693 0.9987 0.924 loss = 3.696\n",
            "Epoch 32 Batch 1537 0.9853 1.0 0.96 loss = 3.615\n",
            "Epoch 32 Batch 2049 0.9733 0.9987 0.928 loss = 3.713\n",
            "Train [32] Time 184.4224190711975 s-acc 0.9747 l-acc 0.9984 m-acc 0.935 err 3.713\n",
            "Valid [32] Time 6.71199631690979 s-acc 0.9561 l-acc 0.9966 m-acc 0.8901 err 3.845\n",
            "saving model\n",
            "Epoch 33 Batch 1 0.9733 0.9987 0.924 loss = 3.706\n",
            "Epoch 33 Batch 513 0.98 0.9973 0.952 loss = 3.709\n",
            "Epoch 33 Batch 1025 0.9707 0.9987 0.924 loss = 3.696\n",
            "Epoch 33 Batch 1537 0.9733 1.0 0.932 loss = 3.782\n",
            "Epoch 33 Batch 2049 0.984 0.9987 0.96 loss = 3.645\n",
            "Train [33] Time 183.74761962890625 s-acc 0.9748 l-acc 0.9985 m-acc 0.9354 err 3.709\n",
            "Valid [33] Time 6.749439001083374 s-acc 0.9644 l-acc 0.9974 m-acc 0.9103 err 3.755\n",
            "saving model\n",
            "Epoch 34 Batch 1 0.9773 1.0 0.944 loss = 3.672\n",
            "Epoch 34 Batch 513 0.976 0.9987 0.94 loss = 3.673\n",
            "Epoch 34 Batch 1025 0.9827 1.0 0.952 loss = 3.628\n",
            "Epoch 34 Batch 1537 0.968 0.9947 0.924 loss = 3.737\n",
            "Epoch 34 Batch 2049 0.9707 0.9973 0.924 loss = 3.784\n",
            "Train [34] Time 186.76560282707214 s-acc 0.9756 l-acc 0.9985 m-acc 0.9374 err 3.697\n",
            "Valid [34] Time 6.75471305847168 s-acc 0.969 l-acc 0.9979 m-acc 0.9222 err 3.702\n",
            "saving model\n",
            "Epoch 35 Batch 1 0.9747 1.0 0.924 loss = 3.615\n",
            "Epoch 35 Batch 513 0.9867 0.9987 0.964 loss = 3.592\n",
            "Epoch 35 Batch 1025 0.9693 0.9973 0.924 loss = 3.776\n",
            "Epoch 35 Batch 1537 0.98 1.0 0.944 loss = 3.649\n",
            "Epoch 35 Batch 2049 0.9787 0.9987 0.94 loss = 3.664\n",
            "Train [35] Time 183.87690591812134 s-acc 0.9761 l-acc 0.9985 m-acc 0.9389 err 3.69\n",
            "Valid [35] Time 6.869636535644531 s-acc 0.9491 l-acc 0.9953 m-acc 0.8709 err 3.931\n",
            "saving model\n",
            "Epoch 36 Batch 1 0.964 0.996 0.908 loss = 3.853\n",
            "Epoch 36 Batch 513 0.9693 0.9973 0.916 loss = 3.765\n",
            "Epoch 36 Batch 1025 0.976 0.9987 0.944 loss = 3.683\n",
            "Epoch 36 Batch 1537 0.98 0.9987 0.948 loss = 3.638\n",
            "Epoch 36 Batch 2049 0.9787 0.9987 0.944 loss = 3.637\n",
            "Train [36] Time 185.85605430603027 s-acc 0.9767 l-acc 0.9986 m-acc 0.9404 err 3.683\n",
            "Valid [36] Time 7.468310117721558 s-acc 0.9658 l-acc 0.9976 m-acc 0.9137 err 3.735\n",
            "saving model\n",
            "Epoch 37 Batch 1 0.9893 1.0 0.972 loss = 3.555\n",
            "Epoch 37 Batch 513 0.9707 0.9987 0.92 loss = 3.715\n",
            "Epoch 37 Batch 1025 0.9747 1.0 0.936 loss = 3.691\n",
            "Epoch 37 Batch 1537 0.9693 0.9987 0.924 loss = 3.742\n",
            "Epoch 37 Batch 2049 0.9827 0.9987 0.952 loss = 3.602\n",
            "Train [37] Time 187.44970393180847 s-acc 0.9772 l-acc 0.9986 m-acc 0.9418 err 3.674\n",
            "Valid [37] Time 6.721792936325073 s-acc 0.9551 l-acc 0.9959 m-acc 0.8822 err 3.874\n",
            "saving model\n",
            "Epoch 38 Batch 1 0.9773 0.9973 0.94 loss = 3.696\n",
            "Epoch 38 Batch 513 0.988 1.0 0.964 loss = 3.57\n",
            "Epoch 38 Batch 1025 0.9733 0.9987 0.92 loss = 3.724\n",
            "Epoch 38 Batch 1537 0.972 0.9987 0.932 loss = 3.734\n",
            "Epoch 38 Batch 2049 0.988 1.0 0.964 loss = 3.556\n",
            "Train [38] Time 184.1889569759369 s-acc 0.9775 l-acc 0.9986 m-acc 0.9425 err 3.672\n",
            "Valid [38] Time 6.677125930786133 s-acc 0.9398 l-acc 0.9937 m-acc 0.8487 err 4.053\n",
            "saving model\n",
            "Epoch 39 Batch 1 0.9747 0.9987 0.928 loss = 3.692\n",
            "Epoch 39 Batch 513 0.9813 1.0 0.952 loss = 3.603\n",
            "Epoch 39 Batch 1025 0.9773 0.9973 0.944 loss = 3.669\n",
            "Epoch 39 Batch 1537 0.988 1.0 0.968 loss = 3.53\n",
            "Epoch 39 Batch 2049 0.9747 1.0 0.948 loss = 3.665\n",
            "Train [39] Time 186.12545680999756 s-acc 0.9782 l-acc 0.9986 m-acc 0.9443 err 3.661\n",
            "Valid [39] Time 6.726463317871094 s-acc 0.9718 l-acc 0.998 m-acc 0.929 err 3.666\n",
            "saving model\n",
            "Epoch 40 Batch 1 0.984 0.9973 0.956 loss = 3.653\n",
            "Epoch 40 Batch 513 0.98 1.0 0.956 loss = 3.607\n",
            "Epoch 40 Batch 1025 0.976 1.0 0.936 loss = 3.665\n",
            "Epoch 40 Batch 1537 0.9733 1.0 0.928 loss = 3.642\n",
            "Epoch 40 Batch 2049 0.976 0.9987 0.932 loss = 3.675\n",
            "Train [40] Time 184.9380407333374 s-acc 0.9784 l-acc 0.9986 m-acc 0.9451 err 3.657\n",
            "Valid [40] Time 6.670702934265137 s-acc 0.9582 l-acc 0.9965 m-acc 0.8939 err 3.828\n",
            "saving model\n",
            "Epoch 41 Batch 1 0.9773 0.996 0.944 loss = 3.711\n",
            "Epoch 41 Batch 513 0.968 0.996 0.928 loss = 3.754\n",
            "Epoch 41 Batch 1025 0.9867 1.0 0.964 loss = 3.595\n",
            "Epoch 41 Batch 1537 0.976 1.0 0.928 loss = 3.656\n",
            "Epoch 41 Batch 2049 0.976 1.0 0.936 loss = 3.627\n",
            "Train [41] Time 183.7013738155365 s-acc 0.979 l-acc 0.9986 m-acc 0.9465 err 3.65\n",
            "Valid [41] Time 7.6867547035217285 s-acc 0.9556 l-acc 0.9962 m-acc 0.8847 err 3.86\n",
            "saving model\n",
            "Epoch 42 Batch 1 0.984 0.9987 0.96 loss = 3.565\n",
            "Epoch 42 Batch 513 0.9787 1.0 0.956 loss = 3.663\n",
            "Epoch 42 Batch 1025 0.9813 1.0 0.944 loss = 3.6\n",
            "Epoch 42 Batch 1537 0.988 1.0 0.968 loss = 3.543\n",
            "Epoch 42 Batch 2049 0.9747 0.9973 0.932 loss = 3.714\n",
            "Train [42] Time 185.6263358592987 s-acc 0.9792 l-acc 0.9987 m-acc 0.9471 err 3.646\n",
            "Valid [42] Time 6.694947004318237 s-acc 0.9667 l-acc 0.9976 m-acc 0.9148 err 3.724\n",
            "saving model\n",
            "Epoch 43 Batch 1 0.9773 1.0 0.948 loss = 3.65\n",
            "Epoch 43 Batch 513 0.9787 0.9973 0.952 loss = 3.652\n",
            "Epoch 43 Batch 1025 0.9747 0.996 0.936 loss = 3.717\n",
            "Epoch 43 Batch 1537 0.9733 0.9987 0.944 loss = 3.708\n",
            "Epoch 43 Batch 2049 0.9787 0.996 0.952 loss = 3.693\n",
            "Train [43] Time 185.16067838668823 s-acc 0.9797 l-acc 0.9987 m-acc 0.9486 err 3.639\n",
            "Valid [43] Time 7.1738975048065186 s-acc 0.9504 l-acc 0.9949 m-acc 0.8762 err 3.912\n",
            "saving model\n",
            "Epoch 44 Batch 1 0.98 0.9987 0.948 loss = 3.636\n",
            "Epoch 44 Batch 513 0.988 1.0 0.964 loss = 3.583\n",
            "Epoch 44 Batch 1025 0.976 0.9987 0.94 loss = 3.664\n",
            "Epoch 44 Batch 1537 0.9773 0.996 0.944 loss = 3.7\n",
            "Epoch 44 Batch 2049 0.98 0.9987 0.944 loss = 3.627\n",
            "Train [44] Time 188.18404006958008 s-acc 0.98 l-acc 0.9987 m-acc 0.9494 err 3.633\n",
            "Valid [44] Time 6.707849502563477 s-acc 0.9638 l-acc 0.9972 m-acc 0.9099 err 3.746\n",
            "saving model\n",
            "Epoch 45 Batch 1 0.984 1.0 0.96 loss = 3.633\n",
            "Epoch 45 Batch 513 0.984 0.9987 0.956 loss = 3.612\n",
            "Epoch 45 Batch 1025 0.976 1.0 0.932 loss = 3.604\n",
            "Epoch 45 Batch 1537 0.98 1.0 0.952 loss = 3.652\n",
            "Epoch 45 Batch 2049 0.9907 1.0 0.972 loss = 3.54\n",
            "Train [45] Time 185.84610557556152 s-acc 0.9805 l-acc 0.9988 m-acc 0.9505 err 3.627\n",
            "Valid [45] Time 6.684953212738037 s-acc 0.9835 l-acc 0.9987 m-acc 0.96 err 3.534\n",
            "saving model\n",
            "Epoch 46 Batch 1 0.9907 1.0 0.972 loss = 3.505\n",
            "Epoch 46 Batch 513 0.9787 0.9973 0.944 loss = 3.685\n",
            "Epoch 46 Batch 1025 0.9867 0.996 0.972 loss = 3.629\n",
            "Epoch 46 Batch 1537 0.9693 0.9973 0.912 loss = 3.745\n",
            "Epoch 46 Batch 2049 0.9933 1.0 0.98 loss = 3.527\n",
            "Train [46] Time 183.6880021095276 s-acc 0.9804 l-acc 0.9987 m-acc 0.9505 err 3.627\n",
            "Valid [46] Time 7.703291654586792 s-acc 0.9636 l-acc 0.9972 m-acc 0.9067 err 3.752\n",
            "saving model\n",
            "Epoch 47 Batch 1 0.9867 0.9987 0.976 loss = 3.572\n",
            "Epoch 47 Batch 513 0.9933 1.0 0.984 loss = 3.501\n",
            "Epoch 47 Batch 1025 0.9867 0.9973 0.968 loss = 3.633\n",
            "Epoch 47 Batch 1537 0.984 0.9987 0.956 loss = 3.58\n",
            "Epoch 47 Batch 2049 0.9853 1.0 0.96 loss = 3.573\n",
            "Train [47] Time 185.9369134902954 s-acc 0.981 l-acc 0.9988 m-acc 0.9519 err 3.618\n",
            "Valid [47] Time 6.680004119873047 s-acc 0.9526 l-acc 0.9949 m-acc 0.8787 err 3.901\n",
            "saving model\n",
            "Epoch 48 Batch 1 0.9787 1.0 0.944 loss = 3.62\n",
            "Epoch 48 Batch 513 0.9693 0.9973 0.936 loss = 3.7\n",
            "Epoch 48 Batch 1025 0.9773 0.9947 0.944 loss = 3.634\n",
            "Epoch 48 Batch 1537 0.9933 1.0 0.984 loss = 3.531\n",
            "Epoch 48 Batch 2049 0.984 0.9987 0.956 loss = 3.612\n",
            "Train [48] Time 185.71075868606567 s-acc 0.9813 l-acc 0.9988 m-acc 0.9527 err 3.614\n",
            "Valid [48] Time 6.7208030223846436 s-acc 0.9724 l-acc 0.9981 m-acc 0.9299 err 3.651\n",
            "saving model\n",
            "Epoch 49 Batch 1 0.9853 0.9987 0.964 loss = 3.582\n",
            "Epoch 49 Batch 513 0.9827 0.9947 0.96 loss = 3.621\n",
            "Epoch 49 Batch 1025 0.972 0.9973 0.932 loss = 3.698\n",
            "Epoch 49 Batch 1537 0.988 1.0 0.964 loss = 3.549\n",
            "Epoch 49 Batch 2049 0.9853 0.9973 0.956 loss = 3.593\n",
            "Train [49] Time 184.6630141735077 s-acc 0.9816 l-acc 0.9988 m-acc 0.9534 err 3.609\n",
            "Valid [49] Time 6.688522577285767 s-acc 0.9823 l-acc 0.9985 m-acc 0.9571 err 3.546\n",
            "saving model\n",
            "Epoch 50 Batch 1 0.976 1.0 0.932 loss = 3.615\n",
            "Epoch 50 Batch 513 0.9813 0.9987 0.952 loss = 3.616\n",
            "Epoch 50 Batch 1025 0.9707 1.0 0.92 loss = 3.717\n",
            "Epoch 50 Batch 1537 0.972 0.9987 0.94 loss = 3.652\n",
            "Epoch 50 Batch 2049 0.9853 0.9973 0.964 loss = 3.574\n",
            "Train [50] Time 184.85948586463928 s-acc 0.9818 l-acc 0.9988 m-acc 0.954 err 3.607\n",
            "Valid [50] Time 6.67274284362793 s-acc 0.9611 l-acc 0.9966 m-acc 0.9022 err 3.782\n",
            "saving model\n",
            "Epoch 51 Batch 1 0.9707 0.9947 0.928 loss = 3.752\n",
            "Epoch 51 Batch 513 0.972 0.9973 0.94 loss = 3.752\n",
            "Epoch 51 Batch 1025 0.9907 1.0 0.976 loss = 3.531\n",
            "Epoch 51 Batch 1537 0.984 0.996 0.968 loss = 3.572\n",
            "Epoch 51 Batch 2049 0.9693 0.9987 0.932 loss = 3.717\n",
            "Train [51] Time 184.02906584739685 s-acc 0.9817 l-acc 0.9988 m-acc 0.9541 err 3.607\n",
            "Valid [51] Time 7.15731406211853 s-acc 0.9505 l-acc 0.9948 m-acc 0.8744 err 3.917\n",
            "saving model\n",
            "Epoch 52 Batch 1 0.9853 0.9987 0.96 loss = 3.591\n",
            "Epoch 52 Batch 513 0.984 1.0 0.96 loss = 3.537\n",
            "Epoch 52 Batch 1025 0.984 1.0 0.956 loss = 3.593\n",
            "Epoch 52 Batch 1537 0.9867 1.0 0.968 loss = 3.551\n",
            "Epoch 52 Batch 2049 0.9773 1.0 0.952 loss = 3.619\n",
            "Train [52] Time 185.24121522903442 s-acc 0.9822 l-acc 0.9988 m-acc 0.9551 err 3.601\n",
            "Valid [52] Time 6.6773505210876465 s-acc 0.9596 l-acc 0.9961 m-acc 0.8974 err 3.802\n",
            "saving model\n",
            "Epoch 53 Batch 1 0.9853 1.0 0.96 loss = 3.547\n",
            "Epoch 53 Batch 513 0.9907 1.0 0.972 loss = 3.503\n",
            "Epoch 53 Batch 1025 0.9773 0.9987 0.948 loss = 3.631\n",
            "Epoch 53 Batch 1537 0.988 1.0 0.972 loss = 3.504\n",
            "Epoch 53 Batch 2049 0.972 0.9987 0.92 loss = 3.663\n",
            "Train [53] Time 185.12438797950745 s-acc 0.9826 l-acc 0.9989 m-acc 0.9562 err 3.596\n",
            "Valid [53] Time 6.712782144546509 s-acc 0.9614 l-acc 0.9966 m-acc 0.9042 err 3.779\n",
            "saving model\n",
            "Epoch 54 Batch 1 0.9893 1.0 0.972 loss = 3.573\n",
            "Epoch 54 Batch 513 0.9787 0.9987 0.948 loss = 3.581\n",
            "Epoch 54 Batch 1025 0.98 1.0 0.94 loss = 3.579\n",
            "Epoch 54 Batch 1537 0.98 1.0 0.964 loss = 3.649\n",
            "Epoch 54 Batch 2049 0.98 0.9987 0.944 loss = 3.585\n",
            "Train [54] Time 184.1844961643219 s-acc 0.983 l-acc 0.9989 m-acc 0.9573 err 3.59\n",
            "Valid [54] Time 6.824573755264282 s-acc 0.9561 l-acc 0.996 m-acc 0.8868 err 3.844\n",
            "saving model\n",
            "Epoch 55 Batch 1 0.9867 1.0 0.964 loss = 3.615\n",
            "Epoch 55 Batch 513 0.9853 1.0 0.964 loss = 3.532\n",
            "Epoch 55 Batch 1025 0.988 0.9987 0.972 loss = 3.591\n",
            "Epoch 55 Batch 1537 0.984 1.0 0.956 loss = 3.559\n",
            "Epoch 55 Batch 2049 0.984 1.0 0.96 loss = 3.563\n",
            "Train [55] Time 184.17382264137268 s-acc 0.9829 l-acc 0.9989 m-acc 0.9571 err 3.59\n",
            "Valid [55] Time 6.6445653438568115 s-acc 0.9621 l-acc 0.9967 m-acc 0.905 err 3.771\n",
            "saving model\n",
            "Epoch 56 Batch 1 0.9827 1.0 0.952 loss = 3.571\n",
            "Epoch 56 Batch 513 0.9853 0.9973 0.968 loss = 3.576\n",
            "Epoch 56 Batch 1025 0.984 1.0 0.964 loss = 3.564\n",
            "Epoch 56 Batch 1537 0.992 0.9973 0.984 loss = 3.543\n",
            "Epoch 56 Batch 2049 0.98 0.9987 0.952 loss = 3.568\n",
            "Train [56] Time 184.23738026618958 s-acc 0.9834 l-acc 0.9989 m-acc 0.9581 err 3.583\n",
            "Valid [56] Time 7.848326921463013 s-acc 0.965 l-acc 0.9971 m-acc 0.9123 err 3.732\n",
            "saving model\n",
            "Epoch 57 Batch 1 0.9893 1.0 0.972 loss = 3.546\n",
            "Epoch 57 Batch 513 0.9867 0.9987 0.968 loss = 3.564\n",
            "Epoch 57 Batch 1025 0.9813 0.9987 0.964 loss = 3.615\n",
            "Epoch 57 Batch 1537 0.9747 0.9973 0.948 loss = 3.642\n",
            "Epoch 57 Batch 2049 0.9653 0.9987 0.916 loss = 3.738\n",
            "Train [57] Time 184.75255608558655 s-acc 0.9835 l-acc 0.9989 m-acc 0.9585 err 3.582\n",
            "Valid [57] Time 6.694303512573242 s-acc 0.9719 l-acc 0.9977 m-acc 0.9288 err 3.651\n",
            "saving model\n",
            "Epoch 58 Batch 1 0.9933 1.0 0.98 loss = 3.473\n",
            "Epoch 58 Batch 513 0.9827 0.9987 0.964 loss = 3.622\n",
            "Epoch 58 Batch 1025 0.9853 1.0 0.96 loss = 3.529\n",
            "Epoch 58 Batch 1537 0.9893 1.0 0.968 loss = 3.5\n",
            "Epoch 58 Batch 2049 0.976 1.0 0.932 loss = 3.584\n",
            "Train [58] Time 184.489595413208 s-acc 0.9835 l-acc 0.9989 m-acc 0.9585 err 3.579\n",
            "Valid [58] Time 6.676354169845581 s-acc 0.9773 l-acc 0.9983 m-acc 0.9431 err 3.592\n",
            "saving model\n",
            "Epoch 59 Batch 1 0.9827 0.9987 0.956 loss = 3.572\n",
            "Epoch 59 Batch 513 0.9853 1.0 0.96 loss = 3.582\n",
            "Epoch 59 Batch 1025 0.9813 1.0 0.948 loss = 3.56\n",
            "Epoch 59 Batch 1537 0.9773 1.0 0.948 loss = 3.613\n",
            "Epoch 59 Batch 2049 0.988 1.0 0.964 loss = 3.52\n",
            "Train [59] Time 184.8927984237671 s-acc 0.984 l-acc 0.999 m-acc 0.9598 err 3.573\n",
            "Valid [59] Time 7.659096002578735 s-acc 0.9722 l-acc 0.998 m-acc 0.9291 err 3.652\n",
            "saving model\n",
            "Epoch 60 Batch 1 0.996 1.0 0.988 loss = 3.445\n",
            "Epoch 60 Batch 513 0.98 0.996 0.956 loss = 3.634\n",
            "Epoch 60 Batch 1025 0.9893 1.0 0.968 loss = 3.51\n",
            "Epoch 60 Batch 1537 0.988 0.9973 0.972 loss = 3.563\n",
            "Epoch 60 Batch 2049 0.9867 1.0 0.96 loss = 3.541\n",
            "Train [60] Time 184.98091888427734 s-acc 0.9839 l-acc 0.9989 m-acc 0.9598 err 3.574\n",
            "Valid [60] Time 6.672048568725586 s-acc 0.9803 l-acc 0.9985 m-acc 0.9518 err 3.556\n",
            "saving model\n",
            "Epoch 61 Batch 1 0.984 1.0 0.956 loss = 3.588\n",
            "Epoch 61 Batch 513 0.9893 1.0 0.98 loss = 3.48\n",
            "Epoch 61 Batch 1025 0.98 1.0 0.944 loss = 3.615\n",
            "Epoch 61 Batch 1537 0.984 1.0 0.96 loss = 3.584\n",
            "Epoch 61 Batch 2049 0.9893 1.0 0.972 loss = 3.531\n",
            "Train [61] Time 185.18138241767883 s-acc 0.9843 l-acc 0.9989 m-acc 0.9607 err 3.57\n",
            "Valid [61] Time 6.685698747634888 s-acc 0.9855 l-acc 0.9988 m-acc 0.9651 err 3.501\n",
            "saving model\n",
            "Epoch 62 Batch 1 0.984 0.9987 0.956 loss = 3.623\n",
            "Epoch 62 Batch 513 0.9827 0.9987 0.948 loss = 3.567\n",
            "Epoch 62 Batch 1025 0.996 1.0 0.988 loss = 3.456\n",
            "Epoch 62 Batch 1537 0.9827 1.0 0.956 loss = 3.599\n",
            "Epoch 62 Batch 2049 0.9867 1.0 0.972 loss = 3.512\n",
            "Train [62] Time 185.01866030693054 s-acc 0.9847 l-acc 0.999 m-acc 0.9616 err 3.564\n",
            "Valid [62] Time 6.66838812828064 s-acc 0.9792 l-acc 0.9985 m-acc 0.948 err 3.568\n",
            "saving model\n",
            "Epoch 63 Batch 1 0.9813 0.9987 0.96 loss = 3.62\n",
            "Epoch 63 Batch 513 0.992 1.0 0.976 loss = 3.508\n",
            "Epoch 63 Batch 1025 0.98 0.9987 0.948 loss = 3.608\n",
            "Epoch 63 Batch 1537 0.988 0.9973 0.972 loss = 3.587\n",
            "Epoch 63 Batch 2049 0.984 0.9987 0.96 loss = 3.595\n",
            "Train [63] Time 185.14957356452942 s-acc 0.9846 l-acc 0.999 m-acc 0.9613 err 3.562\n",
            "Valid [63] Time 6.7507641315460205 s-acc 0.9682 l-acc 0.9972 m-acc 0.9183 err 3.698\n",
            "saving model\n",
            "Epoch 64 Batch 1 0.9827 1.0 0.956 loss = 3.549\n",
            "Epoch 64 Batch 513 0.9853 1.0 0.96 loss = 3.558\n",
            "Epoch 64 Batch 1025 0.9947 0.9987 0.984 loss = 3.489\n",
            "Epoch 64 Batch 1537 0.9867 1.0 0.968 loss = 3.518\n",
            "Epoch 64 Batch 2049 0.9893 1.0 0.968 loss = 3.486\n",
            "Train [64] Time 185.69896912574768 s-acc 0.9849 l-acc 0.999 m-acc 0.962 err 3.56\n",
            "Valid [64] Time 7.5603883266448975 s-acc 0.9822 l-acc 0.9985 m-acc 0.9565 err 3.538\n",
            "saving model\n",
            "Epoch 65 Batch 1 0.9893 0.9987 0.976 loss = 3.499\n",
            "Epoch 65 Batch 513 0.988 0.9987 0.964 loss = 3.545\n",
            "Epoch 65 Batch 1025 0.9813 1.0 0.944 loss = 3.57\n",
            "Epoch 65 Batch 1537 0.992 1.0 0.976 loss = 3.485\n",
            "Epoch 65 Batch 2049 0.9933 1.0 0.98 loss = 3.443\n",
            "Train [65] Time 185.2542815208435 s-acc 0.9851 l-acc 0.999 m-acc 0.9624 err 3.556\n",
            "Valid [65] Time 6.633755207061768 s-acc 0.9251 l-acc 0.9885 m-acc 0.8215 err 4.234\n",
            "saving model\n",
            "Epoch 66 Batch 1 0.9853 1.0 0.96 loss = 3.604\n",
            "Epoch 66 Batch 513 0.9907 0.9987 0.976 loss = 3.511\n",
            "Epoch 66 Batch 1025 0.9853 1.0 0.96 loss = 3.548\n",
            "Epoch 66 Batch 1537 0.9907 1.0 0.972 loss = 3.471\n",
            "Epoch 66 Batch 2049 0.984 1.0 0.952 loss = 3.548\n",
            "Train [66] Time 184.3535315990448 s-acc 0.9851 l-acc 0.999 m-acc 0.9625 err 3.555\n",
            "Valid [66] Time 6.660693168640137 s-acc 0.9736 l-acc 0.9979 m-acc 0.9337 err 3.632\n",
            "saving model\n",
            "Epoch 67 Batch 1 0.9867 1.0 0.96 loss = 3.539\n",
            "Epoch 67 Batch 513 0.9813 1.0 0.952 loss = 3.581\n",
            "Epoch 67 Batch 1025 0.984 0.9987 0.968 loss = 3.557\n",
            "Epoch 67 Batch 1537 0.992 1.0 0.976 loss = 3.507\n",
            "Epoch 67 Batch 2049 0.988 0.9973 0.972 loss = 3.576\n",
            "Train [67] Time 185.2245044708252 s-acc 0.9853 l-acc 0.999 m-acc 0.9633 err 3.552\n",
            "Valid [67] Time 6.682720184326172 s-acc 0.9636 l-acc 0.9969 m-acc 0.9077 err 3.748\n",
            "saving model\n",
            "Epoch 68 Batch 1 0.9787 0.9973 0.96 loss = 3.627\n",
            "Epoch 68 Batch 513 0.992 1.0 0.976 loss = 3.473\n",
            "Epoch 68 Batch 1025 0.988 0.9973 0.972 loss = 3.572\n",
            "Epoch 68 Batch 1537 0.9813 1.0 0.952 loss = 3.54\n",
            "Epoch 68 Batch 2049 0.9773 0.9973 0.944 loss = 3.647\n",
            "Train [68] Time 185.00591683387756 s-acc 0.9854 l-acc 0.9991 m-acc 0.9635 err 3.549\n",
            "Valid [68] Time 6.640630006790161 s-acc 0.9793 l-acc 0.9985 m-acc 0.9489 err 3.561\n",
            "saving model\n",
            "Epoch 69 Batch 1 0.9867 1.0 0.972 loss = 3.548\n",
            "Epoch 69 Batch 513 0.9947 1.0 0.984 loss = 3.45\n",
            "Epoch 69 Batch 1025 0.984 1.0 0.96 loss = 3.563\n",
            "Epoch 69 Batch 1537 0.984 1.0 0.956 loss = 3.521\n",
            "Epoch 69 Batch 2049 0.9733 0.996 0.94 loss = 3.698\n",
            "Train [69] Time 184.8003249168396 s-acc 0.9855 l-acc 0.999 m-acc 0.9637 err 3.549\n",
            "Valid [69] Time 6.814737558364868 s-acc 0.9844 l-acc 0.9988 m-acc 0.9614 err 3.504\n",
            "saving model\n",
            "Epoch 70 Batch 1 0.9827 1.0 0.956 loss = 3.559\n",
            "Epoch 70 Batch 513 0.9813 1.0 0.952 loss = 3.553\n",
            "Epoch 70 Batch 1025 0.9893 0.9987 0.972 loss = 3.5\n",
            "Epoch 70 Batch 1537 0.9827 1.0 0.956 loss = 3.581\n",
            "Epoch 70 Batch 2049 0.9933 1.0 0.984 loss = 3.468\n",
            "Train [70] Time 184.54614806175232 s-acc 0.9859 l-acc 0.9991 m-acc 0.9646 err 3.543\n",
            "Valid [70] Time 7.174999475479126 s-acc 0.9864 l-acc 0.999 m-acc 0.9669 err 3.483\n",
            "saving model\n",
            "Epoch 71 Batch 1 0.9933 1.0 0.98 loss = 3.457\n",
            "Epoch 71 Batch 513 0.992 1.0 0.98 loss = 3.472\n",
            "Epoch 71 Batch 1025 0.9973 1.0 0.996 loss = 3.422\n",
            "Epoch 71 Batch 1537 0.9947 1.0 0.984 loss = 3.455\n",
            "Epoch 71 Batch 2049 0.9907 0.9987 0.98 loss = 3.508\n",
            "Train [71] Time 184.98167824745178 s-acc 0.9858 l-acc 0.999 m-acc 0.9646 err 3.545\n",
            "Valid [71] Time 6.704356670379639 s-acc 0.9805 l-acc 0.9985 m-acc 0.9525 err 3.55\n",
            "saving model\n",
            "Epoch 72 Batch 1 0.9773 0.9947 0.956 loss = 3.62\n",
            "Epoch 72 Batch 513 0.9893 1.0 0.968 loss = 3.493\n",
            "Epoch 72 Batch 1025 0.988 1.0 0.964 loss = 3.524\n",
            "Epoch 72 Batch 1537 0.988 1.0 0.972 loss = 3.502\n",
            "Epoch 72 Batch 2049 0.9813 0.9987 0.956 loss = 3.583\n",
            "Train [72] Time 185.0443902015686 s-acc 0.986 l-acc 0.9991 m-acc 0.9651 err 3.54\n",
            "Valid [72] Time 7.047747611999512 s-acc 0.9715 l-acc 0.9979 m-acc 0.9273 err 3.656\n",
            "saving model\n",
            "Epoch 73 Batch 1 0.984 0.9987 0.96 loss = 3.587\n",
            "Epoch 73 Batch 513 0.9867 1.0 0.964 loss = 3.502\n",
            "Epoch 73 Batch 1025 0.988 1.0 0.972 loss = 3.531\n",
            "Epoch 73 Batch 1537 0.9907 0.9973 0.976 loss = 3.497\n",
            "Epoch 73 Batch 2049 0.9867 0.9987 0.96 loss = 3.516\n",
            "Train [73] Time 185.72286534309387 s-acc 0.9861 l-acc 0.9991 m-acc 0.9652 err 3.539\n",
            "Valid [73] Time 6.668141603469849 s-acc 0.9842 l-acc 0.9987 m-acc 0.9612 err 3.511\n",
            "saving model\n",
            "Epoch 74 Batch 1 0.996 1.0 0.988 loss = 3.454\n",
            "Epoch 74 Batch 513 0.9867 0.9973 0.972 loss = 3.545\n",
            "Epoch 74 Batch 1025 0.992 1.0 0.976 loss = 3.46\n",
            "Epoch 74 Batch 1537 0.9787 0.9933 0.948 loss = 3.656\n",
            "Epoch 74 Batch 2049 0.9653 0.9987 0.904 loss = 3.722\n",
            "Train [74] Time 185.26942920684814 s-acc 0.9863 l-acc 0.9991 m-acc 0.9658 err 3.535\n",
            "Valid [74] Time 6.648272514343262 s-acc 0.9836 l-acc 0.9986 m-acc 0.9603 err 3.516\n",
            "saving model\n",
            "Epoch 75 Batch 1 0.9853 1.0 0.96 loss = 3.534\n",
            "Epoch 75 Batch 513 0.992 1.0 0.98 loss = 3.463\n",
            "Epoch 75 Batch 1025 0.9853 1.0 0.972 loss = 3.532\n",
            "Epoch 75 Batch 1537 0.9827 1.0 0.956 loss = 3.576\n",
            "Epoch 75 Batch 2049 0.9947 1.0 0.984 loss = 3.456\n",
            "Train [75] Time 185.47854948043823 s-acc 0.9865 l-acc 0.9991 m-acc 0.9661 err 3.533\n",
            "Valid [75] Time 6.701647043228149 s-acc 0.9702 l-acc 0.9974 m-acc 0.9256 err 3.667\n",
            "saving model\n",
            "Epoch 76 Batch 1 0.9867 0.9987 0.96 loss = 3.537\n",
            "Epoch 76 Batch 513 0.9933 1.0 0.98 loss = 3.475\n",
            "Epoch 76 Batch 1025 0.9813 0.9987 0.968 loss = 3.648\n",
            "Epoch 76 Batch 1537 0.9813 1.0 0.948 loss = 3.573\n",
            "Epoch 76 Batch 2049 0.9893 1.0 0.968 loss = 3.459\n",
            "Train [76] Time 184.60098481178284 s-acc 0.9863 l-acc 0.9991 m-acc 0.9659 err 3.534\n",
            "Valid [76] Time 6.6825621128082275 s-acc 0.984 l-acc 0.9987 m-acc 0.9612 err 3.509\n",
            "saving model\n",
            "Epoch 77 Batch 1 0.9893 1.0 0.968 loss = 3.48\n",
            "Epoch 77 Batch 513 0.9827 0.9973 0.956 loss = 3.564\n",
            "Epoch 77 Batch 1025 0.9813 1.0 0.952 loss = 3.543\n",
            "Epoch 77 Batch 1537 0.988 0.9987 0.976 loss = 3.514\n",
            "Epoch 77 Batch 2049 0.992 1.0 0.976 loss = 3.488\n",
            "Train [77] Time 184.22643995285034 s-acc 0.9864 l-acc 0.9991 m-acc 0.966 err 3.532\n",
            "Valid [77] Time 7.842112064361572 s-acc 0.9722 l-acc 0.9978 m-acc 0.9295 err 3.641\n",
            "saving model\n",
            "Epoch 78 Batch 1 0.9907 1.0 0.972 loss = 3.499\n",
            "Epoch 78 Batch 513 0.9787 0.9987 0.948 loss = 3.612\n",
            "Epoch 78 Batch 1025 0.992 1.0 0.976 loss = 3.449\n",
            "Epoch 78 Batch 1537 0.9853 0.9987 0.964 loss = 3.501\n",
            "Epoch 78 Batch 2049 0.9893 0.9973 0.98 loss = 3.523\n",
            "Train [78] Time 184.198872089386 s-acc 0.9867 l-acc 0.9991 m-acc 0.9669 err 3.528\n",
            "Valid [78] Time 6.668887138366699 s-acc 0.9805 l-acc 0.9986 m-acc 0.9508 err 3.549\n",
            "saving model\n",
            "Epoch 79 Batch 1 0.9907 1.0 0.976 loss = 3.485\n",
            "Epoch 79 Batch 513 0.9933 1.0 0.98 loss = 3.438\n",
            "Epoch 79 Batch 1025 0.9947 1.0 0.984 loss = 3.442\n",
            "Epoch 79 Batch 1537 0.988 1.0 0.968 loss = 3.517\n",
            "Epoch 79 Batch 2049 0.984 1.0 0.956 loss = 3.528\n",
            "Train [79] Time 186.74715614318848 s-acc 0.987 l-acc 0.9991 m-acc 0.9675 err 3.524\n",
            "Valid [79] Time 6.772391319274902 s-acc 0.9855 l-acc 0.9988 m-acc 0.9641 err 3.492\n",
            "saving model\n",
            "Epoch 80 Batch 1 0.992 1.0 0.976 loss = 3.451\n",
            "Epoch 80 Batch 513 0.9853 0.9987 0.96 loss = 3.566\n",
            "Epoch 80 Batch 1025 0.9947 1.0 0.984 loss = 3.438\n",
            "Epoch 80 Batch 1537 0.9813 1.0 0.952 loss = 3.59\n",
            "Epoch 80 Batch 2049 0.9853 0.9973 0.968 loss = 3.56\n",
            "Train [80] Time 186.53470826148987 s-acc 0.987 l-acc 0.9991 m-acc 0.9677 err 3.523\n",
            "Valid [80] Time 6.722359657287598 s-acc 0.9887 l-acc 0.999 m-acc 0.9735 err 3.458\n",
            "saving model\n",
            "Epoch 81 Batch 1 0.9853 1.0 0.96 loss = 3.532\n",
            "Epoch 81 Batch 513 0.9933 1.0 0.984 loss = 3.459\n",
            "Epoch 81 Batch 1025 0.984 0.9987 0.968 loss = 3.586\n",
            "Epoch 81 Batch 1537 0.988 1.0 0.968 loss = 3.513\n",
            "Epoch 81 Batch 2049 0.9853 0.9973 0.968 loss = 3.562\n",
            "Train [81] Time 185.676011800766 s-acc 0.9873 l-acc 0.9991 m-acc 0.9683 err 3.52\n",
            "Valid [81] Time 6.755586624145508 s-acc 0.9689 l-acc 0.997 m-acc 0.9251 err 3.683\n",
            "saving model\n",
            "Epoch 82 Batch 1 0.9947 0.9987 0.992 loss = 3.492\n",
            "Epoch 82 Batch 513 0.988 1.0 0.968 loss = 3.491\n",
            "Epoch 82 Batch 1025 0.988 0.9987 0.964 loss = 3.525\n",
            "Epoch 82 Batch 1537 0.9867 0.9973 0.972 loss = 3.541\n",
            "Epoch 82 Batch 2049 0.9893 0.9987 0.98 loss = 3.503\n",
            "Train [82] Time 188.25523400306702 s-acc 0.9871 l-acc 0.9991 m-acc 0.9679 err 3.521\n",
            "Valid [82] Time 7.384251356124878 s-acc 0.9604 l-acc 0.9955 m-acc 0.9046 err 3.781\n",
            "saving model\n",
            "Epoch 83 Batch 1 0.9947 1.0 0.992 loss = 3.448\n",
            "Epoch 83 Batch 513 0.996 1.0 0.988 loss = 3.431\n",
            "Epoch 83 Batch 1025 0.988 1.0 0.968 loss = 3.485\n",
            "Epoch 83 Batch 1537 0.98 0.9973 0.96 loss = 3.589\n",
            "Epoch 83 Batch 2049 0.9907 1.0 0.976 loss = 3.493\n",
            "Train [83] Time 184.98567819595337 s-acc 0.9874 l-acc 0.9991 m-acc 0.9685 err 3.517\n",
            "Valid [83] Time 6.681696891784668 s-acc 0.9862 l-acc 0.9988 m-acc 0.9665 err 3.484\n",
            "saving model\n",
            "Epoch 84 Batch 1 0.9827 0.996 0.96 loss = 3.586\n",
            "Epoch 84 Batch 513 0.9867 0.9973 0.968 loss = 3.538\n",
            "Epoch 84 Batch 1025 0.984 0.9987 0.964 loss = 3.558\n",
            "Epoch 84 Batch 1537 0.9813 1.0 0.956 loss = 3.542\n",
            "Epoch 84 Batch 2049 0.9893 0.9987 0.976 loss = 3.524\n",
            "Train [84] Time 185.48783206939697 s-acc 0.9875 l-acc 0.9991 m-acc 0.9688 err 3.515\n",
            "Valid [84] Time 6.753500461578369 s-acc 0.9805 l-acc 0.9986 m-acc 0.9506 err 3.546\n",
            "saving model\n",
            "Epoch 85 Batch 1 0.992 1.0 0.98 loss = 3.479\n",
            "Epoch 85 Batch 513 0.9907 1.0 0.976 loss = 3.488\n",
            "Epoch 85 Batch 1025 0.9933 1.0 0.988 loss = 3.46\n",
            "Epoch 85 Batch 1537 0.9693 0.9973 0.932 loss = 3.699\n",
            "Epoch 85 Batch 2049 0.9907 0.9987 0.98 loss = 3.472\n",
            "Train [85] Time 185.4310646057129 s-acc 0.9876 l-acc 0.9992 m-acc 0.9689 err 3.514\n",
            "Valid [85] Time 6.860013008117676 s-acc 0.9855 l-acc 0.9989 m-acc 0.9644 err 3.487\n",
            "saving model\n",
            "Epoch 86 Batch 1 0.9853 0.996 0.976 loss = 3.568\n",
            "Epoch 86 Batch 513 0.988 1.0 0.964 loss = 3.526\n",
            "Epoch 86 Batch 1025 0.992 1.0 0.976 loss = 3.505\n",
            "Epoch 86 Batch 1537 0.9893 1.0 0.976 loss = 3.525\n",
            "Epoch 86 Batch 2049 0.9813 1.0 0.964 loss = 3.549\n",
            "Train [86] Time 185.4528729915619 s-acc 0.9874 l-acc 0.9991 m-acc 0.9686 err 3.514\n",
            "Valid [86] Time 6.719864130020142 s-acc 0.9734 l-acc 0.998 m-acc 0.9322 err 3.627\n",
            "saving model\n",
            "Epoch 87 Batch 1 0.9773 0.9987 0.948 loss = 3.623\n",
            "Epoch 87 Batch 513 0.9867 1.0 0.968 loss = 3.52\n",
            "Epoch 87 Batch 1025 0.996 1.0 0.988 loss = 3.462\n",
            "Epoch 87 Batch 1537 0.9853 0.9987 0.972 loss = 3.517\n",
            "Epoch 87 Batch 2049 0.9907 1.0 0.976 loss = 3.466\n",
            "Train [87] Time 185.10629034042358 s-acc 0.9877 l-acc 0.9992 m-acc 0.9693 err 3.511\n",
            "Valid [87] Time 7.677964687347412 s-acc 0.9816 l-acc 0.9987 m-acc 0.953 err 3.534\n",
            "saving model\n",
            "Epoch 88 Batch 1 0.9867 0.9987 0.968 loss = 3.572\n",
            "Epoch 88 Batch 513 0.9867 1.0 0.968 loss = 3.473\n",
            "Epoch 88 Batch 1025 0.992 1.0 0.976 loss = 3.44\n",
            "Epoch 88 Batch 1537 0.9893 1.0 0.968 loss = 3.494\n",
            "Epoch 88 Batch 2049 0.988 0.9987 0.976 loss = 3.517\n",
            "Train [88] Time 183.9471983909607 s-acc 0.9879 l-acc 0.9991 m-acc 0.9699 err 3.509\n",
            "Valid [88] Time 6.705500841140747 s-acc 0.9896 l-acc 0.9989 m-acc 0.9756 err 3.446\n",
            "saving model\n",
            "Epoch 89 Batch 1 0.9947 1.0 0.984 loss = 3.445\n",
            "Epoch 89 Batch 513 0.9933 1.0 0.984 loss = 3.446\n",
            "Epoch 89 Batch 1025 0.9773 1.0 0.932 loss = 3.627\n",
            "Epoch 89 Batch 1537 0.9947 1.0 0.984 loss = 3.452\n",
            "Epoch 89 Batch 2049 0.984 0.9987 0.96 loss = 3.575\n",
            "Train [89] Time 184.13113260269165 s-acc 0.9878 l-acc 0.9992 m-acc 0.9697 err 3.508\n",
            "Valid [89] Time 6.7264087200164795 s-acc 0.9878 l-acc 0.9989 m-acc 0.9706 err 3.466\n",
            "saving model\n",
            "Epoch 90 Batch 1 0.984 0.9973 0.96 loss = 3.592\n",
            "Epoch 90 Batch 513 0.9853 1.0 0.96 loss = 3.545\n",
            "Epoch 90 Batch 1025 0.9907 1.0 0.976 loss = 3.47\n",
            "Epoch 90 Batch 1537 0.9813 1.0 0.952 loss = 3.514\n",
            "Epoch 90 Batch 2049 0.9933 1.0 0.984 loss = 3.488\n",
            "Train [90] Time 186.4826626777649 s-acc 0.988 l-acc 0.9993 m-acc 0.9701 err 3.504\n",
            "Valid [90] Time 6.681878566741943 s-acc 0.9857 l-acc 0.9989 m-acc 0.9649 err 3.484\n",
            "saving model\n",
            "Epoch 91 Batch 1 0.984 0.9973 0.964 loss = 3.547\n",
            "Epoch 91 Batch 513 0.996 1.0 0.988 loss = 3.414\n",
            "Epoch 91 Batch 1025 0.9893 1.0 0.972 loss = 3.46\n",
            "Epoch 91 Batch 1537 0.9933 0.9987 0.98 loss = 3.496\n",
            "Epoch 91 Batch 2049 0.9893 1.0 0.972 loss = 3.494\n",
            "Train [91] Time 184.2939636707306 s-acc 0.9883 l-acc 0.9992 m-acc 0.9708 err 3.501\n",
            "Valid [91] Time 6.725999593734741 s-acc 0.9603 l-acc 0.996 m-acc 0.8977 err 3.801\n",
            "saving model\n",
            "Epoch 92 Batch 1 0.976 1.0 0.948 loss = 3.603\n",
            "Epoch 92 Batch 513 0.9867 0.9987 0.972 loss = 3.495\n",
            "Epoch 92 Batch 1025 0.9893 1.0 0.968 loss = 3.489\n",
            "Epoch 92 Batch 1537 0.98 0.9987 0.948 loss = 3.554\n",
            "Epoch 92 Batch 2049 0.992 1.0 0.976 loss = 3.457\n",
            "Train [92] Time 185.15424156188965 s-acc 0.9885 l-acc 0.9992 m-acc 0.9712 err 3.499\n",
            "Valid [92] Time 7.190821409225464 s-acc 0.9685 l-acc 0.9972 m-acc 0.9204 err 3.69\n",
            "saving model\n",
            "Epoch 93 Batch 1 0.9867 1.0 0.968 loss = 3.491\n",
            "Epoch 93 Batch 513 0.9947 0.9987 0.988 loss = 3.458\n",
            "Epoch 93 Batch 1025 0.9893 1.0 0.976 loss = 3.46\n",
            "Epoch 93 Batch 1537 0.984 1.0 0.96 loss = 3.514\n",
            "Epoch 93 Batch 2049 0.984 1.0 0.956 loss = 3.531\n",
            "Train [93] Time 185.1513454914093 s-acc 0.9885 l-acc 0.9992 m-acc 0.9713 err 3.497\n",
            "Valid [93] Time 6.678616762161255 s-acc 0.9592 l-acc 0.9953 m-acc 0.8977 err 3.809\n",
            "saving model\n",
            "Epoch 94 Batch 1 0.9893 0.9987 0.968 loss = 3.511\n",
            "Epoch 94 Batch 513 0.9893 1.0 0.976 loss = 3.479\n",
            "Epoch 94 Batch 1025 0.9893 0.9987 0.984 loss = 3.518\n",
            "Epoch 94 Batch 1537 0.9907 1.0 0.976 loss = 3.448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09412d0c66E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}